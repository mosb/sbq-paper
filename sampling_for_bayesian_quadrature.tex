\documentclass{article}
\usepackage{preamble}
%\usepackage{subfig}
\newcommand{\dblspace}{\setlength{\baselineskip}{0.8cm}}
\renewcommand{\pskinny}[2]{p\big(#1|#2\big)}
\usepackage{graphicx} % For figures
\usepackage{subfigure} 
\usepackage{natbib}   % For citations
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage[normalem]{ulem}  % for strikethrough
\usepackage{color} % for comments to each other
\usepackage{comment}
\usepackage{pgfplots}
\usepackage{icml2012}

\newlength\fheight  % For tikz figures
\newlength\fwidth 

% \usepackage[accepted]{icml2012}ยก

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Sampling for Bayesian Quadrature}

\begin{document} 
\twocolumn[
\icmltitle{Sampling for Bayesian Quadrature}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2012
% package.
\icmlauthor{Your Name}{email@yourdomain.edu}
\icmladdress{Your Fantastic Institute,
            314159 Pi St., Palo Alto, CA 94306 USA}
\icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
\icmladdress{Their Fantastic Institute,
            27182 Exp St., Toronto, ON M6H 2T1 CANADA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Bayesian Quadrature, Monte Carlo, Gaussian Processes, Numerical Integration, Model Evidence, Likelihood ratios}

\vskip 0.3in
]

\begin{abstract} 
%We describe a novel approach to quadrature for probabilistic integrals, offering a competitor to traditional Monte Carlo methods. We use a Bayesian quadrature framework \citep{BZHermiteQuadrature,BZMonteCarlo}.
We introduce several innovations making Bayesian Quadrature methods suitable for computing model evidences, or normalization constants.  We demonstrate the many advantages of model-based integration over standard Markov-chain Monte Carlo approaches.  These include a natural stopping criterion, an estimate of uncertainty in our integral, the ability to use active learning rather than markov chains in order to learn about the function being integrated.
\end{abstract} 

\section{Introduction}

Training or evaluating any probabilistic model typically requires an integration over model parameters, weighted by their likelihoods.  This commmon problem has many names:  computing the model evidence[cite skilling?], estimating the partition function, normalizing a distribution.  Typically, this task is performed using Markov-Chain Monte Carlo (MCMC) methods.  These methods have many well-known problems, (such as requiring extensive tuning, becoming stuck in local modes, and falsely appearing to converge \citep{NealMC}).  However, almost all standard approaches fall into the wide faimly of MCMC methods.

These methods estimate the model evidence given the value of the integrand on a set of sample points, a set that is limited in size by the computational expense of evaluating the integrand. As discussed in \citep{MCUnsound}, traditional Monte Carlo integration techniques do not make the best possible use of this valuable information. An alternative is found in Bayesian quadrature \citep{BZHermiteQuadrature}, that uses function samples within a Gaussian process model to compute a closed-formed posterior over the value of the integral.

Bayesian Quadrature has previously been used to infer model evidence, \citep{BZMonteCarlo}, however the approach used suffered from 3 difficulties: 
\begin{itemize}
\item We introduce tractable approximate inference under a log-GP.  In previous work, the \gpb prior used was inappropriate prior for likelihood functions, which are strictly positive and have a high dynamic range.
\item We introduce a tractable approximation which accounts for uncertainty in hyperparameter values.  Previously, uncertainty in the quadrature hyperparameters was ignored, leading to pathologies.
\item We demonstrate how to actively choose function samples in order to minimize the uncertainty in our integral.
\end{itemize}
As well, we compare our BQ approach to standard MCMC techniques on a real scientific problem, where we demonstrate one of the key advantages of BQ: a closed-form expression for the posterior uncertainty in our integral provides a natural stopping criterion for approximate inference.

\section{Monte Carlo Approaches for Estimating $Z$}

Here, we give a brief overview of the standard methods used for computing normalization constants.

\paragraph*{Simple Monte Carlo} is the simplest method.
\paragraph*{Nested Sampling} is not as simple. TODO: Investigate nested sampling.
\paragraph*{Annealed Importance Sampling} is not as simple.
\citep{neal2001annealed}

\section{Gaussian Processes}
Gaussian processes (\gp s) offer a powerful method to perform Bayesian
inference about functions \citep{GPsBook}. A \gpb is defined as a
distribution over the functions $f: \Phi \rightarrow \mathbb{R}$ such
that the distribution over the possible function values on any finite
subset of $\Phi$ is multivariate Gaussian.  For a function $f(\lfv)$,
the prior distribution over its values $\vf$ on a subset
$\vlfv \subset \Phi$ are completely specified by a mean vector
$\vmu$ and covariance matrix $K$
\begin{align*}%\label{eq:GPDefn}
\textstyle
 &\p{\vf}{I} \deq \N{\vf}{\vmu_{f}}{K_{f}}\\
 &\deq\frac{1}{\sqrt{\det{2\pi K_{f}}}}\,\exp \big(-\frac{1}{2}\,(\vf-\vmu_{f})\tra\,K_{f}\inv\,(\vf-\vmu_{f})\big),
\end{align*}
where $I$, the \emph{context}, forms the background knowledge upon which all our probabilities are conditioned. Its ubqiuity leads us to henceforth drop it from explicit representation for notational convenience. The context, $I$, includes prior knowledge of both the
mean and covariance functions, which generate $\vmu_{f}$ and
$K_{f}$ respectively. The prior mean function is chosen as
appropriate for the problem at hand (often a constant), and the
covariance function is chosen to reflect any prior knowledge about the
structure of the function of interest, for example periodicity or
differentiability. In this paper, we'll use Gaussian
covariance functions,
\begin{align} \label{eq:Gaussian_cov_fn}
% K(\vlfv,\vlfv') & \deq \prod_{e=1}^{E} K_e\big(\phi\pha_e,\phi_e'\big)\\
\textstyle
K_{f}(\lfv_1,\lfv_2)& \deq h_f^2\,\N{\lfv_1}{\lfv_2}{w_{f}}.
\end{align} 
Here $h_f$ specifies the output scale (`height') over $f$, while $w_f$ defines a (squared) input scale (`width') over $\lfv$. Note that $\lfv$ itself may be multi-dimensional, in which case $w_f$ must actually be a covariance matrix. For the remainder of the paper, we'll take $w_f$ as diagonal. 
% Our \gpb distribution is specified by various hyperparameters $\theta_e\allv
% e=1,\,\ldots,\,E$, collectively denoted as $\vect{\theta} \deq
% \{\theta_e\allv e=1,\,\ldots,\, E\}$.  $\vect{\theta}$ includes the mean
% function $\vmu$, as well as parameters required by the covariance
% function, input and output scales, amplitudes, periods, etc. as
% needed.

Let us assume we have observations $(\vlfv_s,\vf_s)$ and
are interested in making predictions about the function
valus $f_\star$ at input $\lfv_\star$. We will assume that knowledge of function inputs such as
$\vlfv_s$ and $\lfv_\star$ is incorporated into $I$ (and will hence usually be hidden). With
this information, we have the predictive equations
$$
\pskinny{f\st}{\vf_s} = 
\bN{f\st}
{\meancond{f}{\vlfv_\star}{s}}
{\covcond{f}{\vlfv_\star}{s}}\,,
$$
where we have, for the mean $\novmean{a}{b}\deq \int a\,\pskinny{a}{b} \ud
a$ and variance $\novcov{a}{b}\deq \int
\big(a-\novmean{a}{b}\big)^2\,\pskinny{a}{b} \ud a$,
\begin{align} 
\textstyle
&\meancond{f}{\lfv_\star}{s}
\deq \mean{f_\star}{\vf_s}
\nonumber\\
&= \mu_{f}(\lfv_\star) + 
K_{f}(\lfv_\star,\vlfv_s)
K_{f}(\vlfv_s,\vlfv_s)\inv
\bigl(\vf_s-{\mu}_{f}(\vlfv_s)\bigr) \label{eq:GPMean}
\\
&\covcond{f}{\lfv_\star}{s}
\deq {\cov{f_\star}{\vf_s}} 
\nonumber\\
&= K_{f}(\lfv_\star,\lfv_\star) - 
K_{f}(\lfv_\star,\vlfv_s)
K_{f}(\vlfv_s,\vlfv_s)\inv
K_{f}(\vlfv_s,\lfv_\star)\,. \nonumber%\label{eq:GPCov}
\end{align} 

\textcolor{red}{[We need to stress that m and C are always gotten by taking expectations over possible functions.  This was a really confusing point to me.]}

\section{Bayesian Quadrature} \label{sec:BQ}

% Note that maximum likelihood is also subject to issues. $\p{D}{\lfv,I}$, how come (known) $I$ is on the right and (known) $D$ is on the left? 

\emph{Bayesian quadrature} \citep{BZHermiteQuadrature,BZMonteCarlo} is a means of
performing Bayesian inference about the value of a potentially
nonanalytic integral 
\begin{equation} \label{eq:intyf}
 \inty{f} \deq \int f(\lfv)\,\po{\lfv}\,\ud\lfv\,.
\end{equation}
%Note that we use a condensed notation; this and all integrals to follow are definite integrals over the entire domain of interest.
We'll assume we are integrating with respect to a Gaussian
prior
\begin{align}\label{eq:phiprior}
\textstyle
 \po{\lfv} & \deq \N{\lfv}{\nu_{\lfv}}{\lambda_{\lfv}} \,,
\end{align}
although other convenient forms, or, if necessary, the use of an
importance re-weighting trick, allow any other integral to be
approximated \citep{OsborneAnon}. If $\lfv$ is a vector, $\nu_{\lfv}$ is a  vector of identical size, and $\lambda_{\lfv}$ an appropriate covariance matrix.

\textcolor{red}{[I'm hoping to re-write the next two paragraphs; it's way too indirect, and I think the philosophical foundations of MCMC are a poor avenue of attack]}
Quadrature involves evaluating $f(\lfv)$ at a
vector of sample points $\vlfv_s$, giving $\vf\pha_s\deq
f(\vlfv_s)$. Of course, this evaluation is usually a computationally expensive
operation.
%---we clearly can't afford to evaluate $f(\lfv)$ for all possible inputs $\lfv$ for an unbounded domain $\lfv$. 
The resultant sparsity of our samples introduces uncertainty about the function $f$ between them, and hence uncertainty about the integral $\inty{f}$.

As ever in the face of uncertainty, we address the estimation of the value of our integral as a problem of Bayesian inference \citep{BZNumericalAnalysis}. In considering any problem of inference, we need to be clear about both what information we have and which uncertain variables we are interested in. In our case, both the values $f(\vlfv_s)$ and their locations $\vlfv_s$ represent valuable pieces of knowledge. As discussed by \citet{MCUnsound}, traditional Monte Carlo, which approximates as
\begin{equation} \label{eq:MC_integral_estimate}
\inty{f} \simeq \frac{1}{\card{s}} \sum_{i=1}^{\card{s}} f(\lfv_i)\,,
\end{equation}
effectively ignores the information content of $\vlfv_s$, leading to unsatisfactory behaviour
%\footnote{  For example, imagine that we had $\card{s}=3$, and $\lfv_1 = \lfv_2$. In this case, the identical value $q(\lfv_1)= q(\lfv_2)$ will receive $\nicefrac{2}{3}$ of the weight, whereas the equally useful $q(\lfv_3)$ will receive only $\nicefrac{1}{3}$. \textcolor{red}{I'm not crazy about this argument, since the weightings are also incorporating information about the prior...} }
.

% As with our convention above, we will take knowledge of
% sample locations $\vlfv_s$ to be implicit within $I$. However, as we don't know $f(\lfv)$ for any $\lfv \not \in \vlfvS$, we are uncertain about the function $f(\cdot)$. As a consequence, we are also uncertain about the value of the integral $\inty{f}$. As such, we possess probability distributions over both $f(\cdot)$ and $\inty{f}$. 
% %The resulting Bayesian network is depicted in Figure \ref{fig:BMC}.

%  \begin{figure}[ht]
% \hspace{-1cm}
% 	\begin{pspicture}(-5,0)(5,4.25)%
% 	%\showgrid
% 	\GM@Inode{0}{3.5}{1}%	
% 	%\rput(I){\rput(0,-2){\GM@node{X}}}   \GM@label[angle=90]{X}{$X$}
% 	\rput(0,2){\GM@detnode{psi}}   \GM@label[angle=-90]{psi}{$\rv{\inty{f}}$}
% 
% % NB \lfv is the actual value of the hyperparameters -- doesn't make any sense to write \lfv_i
% 
% 	\rput(psi){\rput(1.25,-2){\GM@plate[plateLabelPos=bl]{2}{4.2}{$i'\not\in s$}}}
% 	\rput(psi){\rput(2.5,1){\GM@node[observed=true]{phij}}}   \GM@label[angle=90]{phij}{$\rv{\lfv}_{i'}$}
% 	\rput(phij){\rput(0,-2){\GM@detnode{qj}}}   \GM@label[angle=130]{qj}{$\rv{f}_{i'}$}
% 
% 	\rput(psi){\rput(-3.25,-2){\GM@plate[plateLabelPos=br]{2}{4.2}{$i \in s$}}}
% 	\rput(psi){\rput(-2.5,1){\GM@node[observed=true]{phii}}}   \GM@label[angle=90]{phii}{$\rv{\lfv}_i$}
% 	\rput(phii){\rput(0,-2){\GM@detnode[observed=true]{qi}}}   \GM@label[angle=50]{qi}{$\rv{f}_i$}
% 
% 	\pnode(0,0.5){mid}
% 
% 	%\ncline[arrows=->]{phi}{X}
% 
% 
% 	\ncline[arrows=->]{phij}{qj}
% 	\ncline[arrows=->]{qj}{psi}
% 	\ncline[arrows=->]{phij}{psi}
% 
% 	\ncline[arrows=->]{phii}{qi}
% 	\ncline[arrows=->]{qi}{psi}
% 	\ncline[arrows=->]{phii}{psi}
% 
% 	\ncarc{qj}{qi}
% 	\nccircle[angleA=-90]{qj}{0.5}
% 	\nccircle[angleA=90]{qi}{0.5}
% 
% 	\end{pspicture}%
% \caption{Bayesian network for Bayesian Quadrature.}
% \label{fig:BMC}
% \end{figure}

We choose for $f$ a 
\gpb prior with mean $\mu_f$ and the Gaussian covariance function \eqref{eq:Gaussian_cov_fn}.
Here the scales $h_f$ and $w_f$ are \emph{quadrature hyperparameters}, hyperparameters that specify the
 \gpb used for Bayesian quadrature. These scales, and the others that follow, will be taken as given and incorporated into the (hidden) context $I$.

% Many more of these will be implicitly introduced in the coming sections; we'll take this as given and incorporate them into the (hidden) context $I$. 
% Note that it will later become apparent that our inference for $\inty{f}$ is independent of the $h_f$ quadrature hyperparameter.

Note that variables over which we have a multivariate Gaussian distribution are jointly Gaussian distributed with any projections of those variables. Because integration is a projection, we can hence use our computed samples $\vf_s$ to perform analytic Gaussian process inference about the value of integrals over $f(\lfv)$, such as $\inty{f}$. Our mean estimate for $\inty{f}$ given $\vf_s$ is

\begin{align} \label{eq:mean_inty_f}
\mean{\inty{f}}{\vf_s}
& 
=\iint \inty{f}\,\p{\inty{f}}{f}\p{f}{\vf_s} \ud \inty{f} \,\ud f                                                                                                                                                               \nonumber\\
&
 =\iint \inty{f}\,\dd{\inty{f}}{\int f(\lfv)\,\po{\lfv}\,\ud\lfv}
\nonumber\\
&\hspace{2.5cm}
\N{f}{\meancondfn{f}{s}}{\covcondfn{f}{s}} \ud \inty{f} \,\ud f \nonumber\\
&
 = \int \meancondfn{f}{s}(\lfv)\,\po{\lfv}\,\ud\lfv\nonumber\\
&
 = 
%\N{\inty{f}}
\mu_f + \ntT{s}{f}\, \dtt{s}{f}
%{\varpi_{f}-\ntT{s}{f} K_{f}(\vlfv_s,\vlfv_s)\inv \nt{s}{f}}
\,,
\end{align}
where for $\lfv_i \in \vlfv_s$,
\begin{align*}
\fnt{\lfv_i}{f} & \deq \!\int K_f(\lfv_i,\lfv)\po{\lfv}\ud\lfv
 = h_f^2\,\N{\lfv_i}{\nu_{\lfv}}{\lambda_{\lfv}+w_{f}}\\
%\intertext{and}
\dtt{s}{f} & \deq K_{f}\bigl(\vlfv_s,\vlfv_s\bigr)\inv (\vf_s-{\mu}_{f})\,.
\end{align*}
Note that the form of our `best estimate' for $\inty{f}$, \eqref{eq:mean_inty_f}, is an affine combination of the samples $\vf_s$, just as for traditional quadrature or Monte Carlo techniques. 
% Indeed, if $\mu_f$ is taken as the mean of $\vf_s$ (as is usual for \gpb inference), the second term in \eqref{eq:mean_inty_f} can be viewed as a correction factor to the Monte Carlo estimate \eqref{eq:MC_integral_estimate}. \textcolor{red}{[Since we're using a different sampling strategy than MCMC in this paper, I think the preceeding statement is kind of misleading / confusing...]}
% \sout{Note also that $h_f$ represents a simple multiplicative factor to both $\ntT{s}{f}$ and $K_{f}\bigl(\vlfv_s,\vlfv_s\bigr)$, and as such cancels out of \eqref{eq:mean_inty_f}.} 



\section{Modeling Likelihood Functions}

We wish to evaluate the evidence $\inty{\lfn} = \int \lfn(\lfv) p(\lfv) \ud\lfv$, an integral over non-negative likelihoods, $\lfn(\lfv)$, who arguments are the parameters of the relevant model.

Assigning a standard \gpb prior to $\lfn(\lfv)$ would ignore our prior information about the range and non-negativity of $\lfn(\lfv)$, leading to pathologies such as a negative evidence (as observed in \citep{BZMonteCarlo}).  A much better [citation needed, or maybe a diagram.  Maybe we should mention that most likelihood functions are exps of something] prior would be a \gpb prior on $\log(\lfn(x))$.  However, the integral under the exp of a \gpb is intractable.  In this section, we introduce an approximate inference method to tractably integrate under a log-\gpb prior\footnote{In practice, we use the transform 
$\log\left(\nicefrac{\lfn(\lfv)}{\gamma} + 1\right)$
where $\gamma \deq 100\, \max(\vr_s)$.  This give better resolution of some numerical issues, and allows us to assume the transformed quantity has zero mean. For the sake of simplicity, we omit this detail in the following derivations.}.
%Firstly, we define the transformation
%\begin{align*}

% \intertext{and its inverse}
% t\inv\bigl(\tr(\lfv)\bigr) & \deq \gamma \bigl(\exp \tr(\lfv)-1\bigr)
%\end{align*}
% and their inverses
% $$
% t_q\inv\bigl(\tq(\lfv)\bigr) \deq \gamma\bigl(\eta+\exp \tq(\lfv)\bigr)
% \qquad \text{and} \qquad
% t_q\inv\bigl(\tq(\lfv)\bigr) \deq \gamma\bigl(\eta+\exp \tq(\lfv)\bigr)\,,
% $$
%(which has well-defined inverse $t\inv$) where 
%$
% \gamma \deq 100\, \max(\vr_s)
%$
%is a constant introduced for superior numerical accuracy.
%
%We now assign \gpb priors to the functions
%\begin{align*}
% \tr & \deq  t(r)
%\end{align*}
% We assume that far away from our existing observations,
% the means for $q$ and $r$ are equal to a value suitably close to
% zero, and hence take a corresponding zero prior mean for the \gp s over $\tq$ and $\tr$. 
%with Gaussian
%covariances of the form \eqref{eq:Gaussian_cov_fn}.
%the minimum of the existing observations.
%These choices of prior distribution are motivated by the fact that
%$l$ is strictly positive and possesses a large dynamic
%range. 
%
%
\begin{align}
\mean{\inty{\lfn}}{\vr_s}
% & 
% =\iint \inty{l}\,\p{\inty{l}}{l}\p{l}{\vl_s} \ud \inty{l} \,\ud l                                                                                                                                                               \nonumber\\
% &
&  =\int \Bigl( \int \lfn(\lfv)\po{\lfv}\,\ud\lfv\Bigr)
\N{\tr}{\meancondfn{\tr}{s}}{\covcondfn{\tr}{s}} \ud \tr \nonumber
\end{align}

Here the exponential ruins the linearity that yielded the analyticity in section \ref{sec:BQ}. To combat this, we perform inference for $\inty{\lfn}$ directly as a functional of $\tr$.
%
\begin{align}
 \psi[\tr] \deq \inty{\lfn} & = \int \exp ( \tr(\lfv)) p(\lfv) \ud \lfv\\
  						    & = \int \lfn(\lfv) p(\lfv) \ud \lfv\\
\pderiv{}{\tr(\lfv)}\psi[\tr] & = \exp (\tr(\lfv)) p(\lfv)  = \lfn(\lfv) p(\lfv) 
\end{align}

We make a \emph{linearisation} approximation\footnote{Note that this linearisation is equivalent to taking another \gpb for $\psi$. with the affine covariance
\begin{equation*}
 K_\psi(\tr,\tr')
% \deq 
%  K_\psi\bigl((\tvq\pha_c,\tvr\pha_c),(\tvq'_c,\tvr'_c)\bigr)
\deq
\int\tr(\lfv) \tr'(\lfv) \ud \lfv
+ \omega^2\,.
\end{equation*}
} 
for $\psi$, forcing $\psi$ to be, as desired, affine in $\tr$. 
%\textcolor{red}{[let's talk a bit about what the functional looks like without taking the linearization approximation maybe?]}

Before proceeding, we introduce a separate \gpb model over $\lfn$, the non-log space.  Then $m_{\lfn|s}$ is the \gpb conditional mean (as per \eqref{eq:GPMean}) for $\lfn$ given observations $\lfn(\vlfv_s)$. For this \gpb (over the non-log space), we take zero prior means and Gaussian
covariances of the form \eqref{eq:Gaussian_cov_fn}. 

We perform the linearisation of $\psi[\tr]$ around the point defined by $\tr_0 \deq \log (m_{\lfn|s})$. We make the definitions 
$\psi_0 \deq \psi[\tr_0]$, $\pderiv{\psi_0}{\tr(\lfv)} \deq \pderiv{\psi}{\tr(\lfv)}[\tr_0]$, giving us
\begin{multline}\label{eq:linearisation}
\novmean{\psi[\tr]}{\psi_0,\pderiv{\psi_0}{\tq(\lfv)}} 
= \psi_0+\\
\int \pderiv{\psi_0}{\tr(\lfv)}\bigl(\tr(\lfv)-\tr_0(\lfv)\bigr)\ud\lfv\,.
\end{multline}


We now define
\begin{align*}
\Delta & \deq m_{\tr |s} - \log(m_{r|s}) = m_{\tr |s}  - \tr_0 \,,
\end{align*}
%\textcolor{red}{[You know, until I did the notation change, I never understood that $\Delta$ was defined on the logs, not on the untransformed quantities.]} 
the difference between the \gpb means over our log-transformed quantities and the transformed \gpb means over original quantities. 
%\textcolor{red}{[I'd like to insert here my figure 2]}. 
%we have
%\begin{align}\label{eq:mt_sim_tm}
%\Delta \simeq 0\,,
%\end{align}
%\textcolor{red}{[I found this statement confusing, it looks like we're planning to ignore $\Delta$ instead of giving it a full treatment.]}
We expect $\Delta$ to be small for the narrow range of $\tr$ functions permitted by our Gaussian over $\tr$. This implies that
 $\tr_0$ is close to the peaks of our Gaussians over $\tr$, rendering our linearisation appropriate. The choice of $\tr_0$ is motivated by the fact that for it, our integrals become tractable.

% Our linearisation corresponds to giving our \gpb over $\psi$ observations
% at $\tr_0= \log (m_{r|s})$ of both the functional itself,
That is, we can now write
\begin{align}
\psi_0 & = \psi[\tr_0]
= 
{\mean{\inty{\lfn}}{\vr_s}} \label{eq:rho_0}
\intertext{along with the functional derivative}
\pderiv{\psi_0}{\tr(\lfv)} & = \pderiv{\psi}{\tr(\lfv)}[\tr_0]
 = \mean{r(\lfv)}{\vr_s}\,\po{\lfv}
\nonumber
\end{align}
As with any linearisation approximation, \eqref{eq:linearisation} gives the value at the selected point $\tr_0$, plus a correction factor modelling the influence of the first derivatives. This correction factor, 
$\int \pderiv{\psi_0}{\tr(\lfv)}\Delta(\lfv)\ud\lfv$,
contains a further non-analytic integral, due to the $t_r(m_{r|s})$ term within $\Delta$. As such, we perform another stage of Bayesian quadrature by treating $\Delta$ as an unknown function of $\lfv$.

For this function we take Gaussian process priors with zero prior mean and Gaussian covariance \eqref{eq:Gaussian_cov_fn}. We must now choose sample points $\vlfv_c$ at which to evaluate our $\Delta$ function. 
Note that we do not need to evaluate $\lfn(\lfv_c)$ in order to compute $\Delta(\lfv_c)$.
% Unfortunately, it is impractical to resolve this decision problem in the same manner as the selection of $\vlfvS$, our ultimate goal (a topic that receives our full attention in Section \ref{sec:SBQ})).
$\vlfv_c$ should firstly include $\vlfv_s$, at which points we know that $\delta$ is equal to zero. Note firstly that a simple heuristic for determining the `best' samples (in the sense of samples with which to fit a \gp) is to select those samples at extrema. Note that the peaks and troughs of $\delta$ occur at points far-removed from $\vlfv_s$, but no further away than a few input scales. This is where the transformed mean for $r$ is liable to differ most from the mean of the transformed variable (and the appropriate $\Delta$ extremised). That is, we would like to select $\vlfv_c$ as points as far away from all points in $\vlfv_s$ as possible, while still remaining within a few input scales. 
%For convenience, we define the allowable region to be the box defined by the least upper bound of $\vlfv_s$ plus three input scales and the greatest lower bound minus three input scales. 
Finding $\vlfv_c$ hence requires solving the largest empty sphere problem, which we can solve with the use of a Voronoi diagram \citep{Voronoi, shamos1975closest, okabe1997locational}. 
Where this requires computation in excess of that afforded by our allowance, we instead construct a \acro{kd}-tree \citep{bentley1975multidimensional} for our $\vlfv_s$, and select $\vlfv_c$ as the centres of the hyper-rectangles defined by the splitting
hyperplanes of the tree.

For future notational convenience, we assume that if we condition on $\psi_0$, we will condition on its functional derivatives. This allows us to write our mean for  $\inty{\lfn}$
%
\begin{align}
& \mean{\inty{\lfn}}{\psi_0,\tvr_s} \nonumber\\
& \deq \int \mean{\psi[\tr]}{\psi_0,\tr}
\p{\tr}{\tvr_s}\, \ud \tr 
\nonumber\\
& = \mean{\psi[\tr]}{\psi_0,m_{\tr|s}} \nonumber\\
& = \mean{\inty{\lfn}}{\vr_s} + \iint \mean{\lfn(\lfv)}{\vr_s}\,
\mean{\Delta(\lfv)}{\Delta_c}\,\po{\lfv}\ud\lfv
\nonumber\\
& = \mean{\inty{\lfn}}{\vr_s} + \mean{\inty{r \Delta}}{\vr_s, \Delta_c}
\label{eq:mean_ev}
\end{align}

\section{Sampling for Bayesian Quadrature}

We aim to select samples $\lfv_s$ so as to minimise the uncertainty in the evidence. We expect such samples to be useful not just for estimating the evidence, but also for any other related expectations, such as would be required to perform prediction using the model.

The variance in the evidence is
\begin{align*}
\cov{\inty{\lfn}}{\tvr_{s}}
& = \int \inty{\lfn}^2 \p{\inty{\lfn}}{\tvr_s} \ud\inty{\lfn}- \mean{\inty{\lfn}}{\tvr_s}^2\,.
\end{align*}
To resolve the first term, define a new functional
\begin{align*}
 \chi[\tr] \deq \inty{\lfn}^2 & = \gamma^2\bigl(\int  \exp \tr(\lfv) p(\lfv) \ud \lfv-1\bigr)^2\\
\pderiv{}{\tr(\lfv)}\chi[\tr] & = 2\gamma \exp \tr(\lfv) p(\lfv) \bigl(\int  \exp \tr(\lfv) p(\lfv) \ud \lfv-1\bigr)
\end{align*}
which we again linearise around $\tr_0$. Hence, as per the above
\begin{align*}
& \cov{\inty{\lfn}}{\chi_0,\psi_0,\tvr_{s}} = \mean{\inty{\lfn}^2}{\chi_0,\tvr_s} - \mean{\inty{\lfn}}{\psi_0,\tvr_s}^2
% & =  (\mean{\inty{\lfn}}{\vr_s})^2 \\
% & \qquad + 2\mean{\inty{\lfn}}{\vr_s}
% \bigl(\mean{\inty{r \Delta}}{\vr_s} + \gamma\, \mean{\inty{\Delta}}{\vr_s}\bigr)
% \\
% & \qquad - \bigl(
% \mean{\inty{\lfn}}{\vr_s} + \mean{\inty{r \Delta}}{\vr_s} + \gamma\, \mean{\inty{ \Delta}}{\vr_s}
% \bigr)^2
% & = \mean{\inty{\lfn}}{\vr_s}
% \bigl(\mean{\inty{r \Delta}}{\vr_s} + \gamma\, \mean{\inty{\Delta}}{\vr_s}\bigr)
% \\
% & \qquad - \bigl( \mean{\inty{r \Delta}}{\vr_s} + \gamma\, \mean{\inty{ \Delta}}{\vr_s}
% \bigr)^2
% \,.
\end{align*}
where
\begin{align*}
&\mean{\inty{\lfn}^2}{\chi_0,\tvr_s} \\
& = \mean{\chi[\tr]}{\chi_0,m_{\tr|s}} \\
& = (\mean{\inty{\lfn}}{\vr_s})^2 \\
& \hspace{1cm} + 2\mean{\inty{\lfn}}{\vr_s}
\bigl(\mean{\inty{r \Delta}}{\vr_s} + \gamma\, \mean{\inty{\Delta}}{\vr_s}\bigr)
\end{align*}


Consider adding a new sample at $\lfv_a$. The expected variance in our evidence after doing so is
\begin{align}
& \bigl\langle \cov{\inty{\lfn}}{\chi_0,\psi_0,\tvr_{s,a}}\mid \chi_0,\psi_0,\tvr_{s}\bigr\rangle\nonumber\\
& = \mean{\inty{\lfn}^2}{\chi_0,\tvr_s}  - 
\int\mean{\inty{\lfn}}{\psi_0,\tvr_{a,s}}^2\p{\tr_a}{\tvr_s}\ud\tr_a\,.\label{eqn:exp_var}
\end{align}
The first term is independent of the selection of $\lfv_a$ and hence can be safely ignored. Noting that $\int \exp(c\, \tr_a)\, \N{\tr_a;m, \sigma^2} \ud\tr_a = \exp(c\, m + \nicefrac{1}{2}\, c^2 \sigma^2)$, the second term can be resolved analytically for any trial $\lfv_a$ (we omit the laborious details of doing so). With this expression, we now select the $\lfv_a$ that minimises the second term using a numerical optimisation technique. 

\section{Marginalising quadrature hyperparameters}

Hitherto, we have taken the maximum likelihood values $m_\theta$ for the quadrature hyperparameters $\theta$ of the quadrature-\gpb used to model the integrand. Once we have very many samples, this approximation seems reasonable: with lots of data, we can be almost certain about the values of the quadrature hyperparameters. However, in the process of taking samples, we'd like to acknowledge the influence new samples may have on our beliefs about the quadrature-hyperparameters. We'd expect this would further promote exploration, particularly early on, so as to firm up our beliefs about the quadrature-hyperparameters. The quadrature-hyperparameters of most interest are the input scales $w_r$; these hyperparameters can have a very dramatic influence on our fit to a function.

Unfortunately, the dependence of our predictions upon these input scales is complex, ruling out analytic results. However, any approximation we make is likely to improve upon our existing assumption that the posterior for all quadrature-hyperparameters is a delta function! Our approach will be to assume that
\begin{itemize}
 \item that the posterior for $\theta$ is Gaussian with mean equal to the maximum likelihood value $m_\theta$, essentially assuming that our prior is very broad. 
\item The Gaussian's covariance matrix $C_\theta$ is taken as diagonal. For quadrature hyperparameters other than the input scales, we take the appropriate diagonal elements as infinitesimally small, such that our posterior for these parameters is a delta function. For the the diagonal elements corresponding to $w_r$, we use [To be determined]
%the squared input scales $w_s$ of another \gp, fitted to the likelihood $s(w_r)$ of the input scales $w_r$. \textcolor{red}{[Neat idea, but I can think of simple examples where the lengthscale is short but the mass is spread out; it seems to me that it'd be better to first fit a GP, then fit a Gaussian to the \gpb posterior.]}
\item that our predictions for $\tr_a$ have a mean which is linear in $w_r$ around the maximum likelihood values, and a variance which is essentially constant.  
\end{itemize}
 
With these assumptions, we have
\begin{align*}
& \mean{\inty{\lfn}}{\psi_0,\tvr_s} \\
& \deq \iint \mean{\psi[\tr]}{\psi_0,\tr}
\p{\tr}{\tvr_s, \theta}\, \p{\theta}{\tvr_s}\ud \tr\,\ud \theta\\
& = \iint \mean{\psi[\tr]}{\psi_0,\tr} \N{\tr}{m_{\tr|s,\theta}}{C_{\tr|s,\theta}}
\\
& \hspace{5cm}\N{\theta}{m_\theta}{C_\theta}\ud \tr\,\ud \theta
\\
& \simeq \iint \mean{\psi[\tr]}{\psi_0,\tr} 
\\
& \hspace{2cm}\N{\tr}
{m_{\tr|s,m_\theta}+\pderiv{m_{\tr|s,\theta}}{\theta}(\theta-m_\theta)}
{C_{\tr|s,m_\theta}}
\\
& \hspace{4cm}
\N{\theta}{m_\theta}{C_\theta}\ud \tr\,\ud \theta
\\
& = \iint \mean{\psi[\tr]}{\psi_0,\tr} \\
& \hspace{1cm}\N{\tr}
{m_{\tr|s,m_\theta}}
{C_{\tr|s,m_\theta}+\pderiv{m_{\tr|s,\theta}}{\theta}C_\theta\pderiv{m\tra_{\tr|s,\theta}}{\theta}}
\ud \tr
\\
& = \mean{\psi[\tr]}{\psi_0,m_{\tr|s,m_\theta}}\,,
\end{align*}
which is identical to \eqref{eq:mean_ev} (note that we had previously just implicitly assumed that $\theta=m_\theta$). 
\textcolor{red}{[I think we'll be short on space; I'm inclined to summarize the entire previous derivation by one sentence saying that the mean doesn't change.]}
However, for the expected variance after adding a sample, we have
\begin{align}\label{eqn:exp_var_wtheta}
& \bigl\langle \cov{\inty{\lfn}}{\chi_0,\psi_0,\tvr_{s,a}}\mid \chi_0,\psi_0,\tvr_{s}\bigr\rangle
\nonumber\\
& =  \mean{\chi[\tr]}{\chi_0,m_{\tr|s,m_\theta}}  - 
\int\mean{\inty{\lfn}}{\psi_0,\tvr_{a,s}}^2
\nonumber\\
& \hspace{2cm}
\times\N{\tr_a}
{\tilde{m}_a }
{\tilde{C}_a +\pderiv{\tilde{m}_a}{\theta}C_\theta\pderiv{\tilde{m}\tra_a}{\theta}}
\ud\tr_a\,.
\end{align}
where
\begin{align*}
\tilde{m}_a & \deq \mean{\tr_a}{\tvr_s,m_\theta}\\
\tilde{C}_a & \deq \cov{\tr_a}{\tvr_s,m_\theta}\,.
\end{align*}
As with \eqref{eqn:exp_var}, \eqref{eqn:exp_var_wtheta} can be evaluated analytically, and minimised to determine the optimal $\lfv_a$.
Hence our variance for $\tr_a$ has been inflated due to our consideration of the influence of hyperparameters, which we expect to promote additional exploration, as desired.

\section{Experiments}

\begin{figure}
\centering
\begin{tabular}{cc}
	\setlength\fheight{2.5cm} 
	\setlength\fwidth{2.5cm}
	\input{figures/integrands/sanity_easy_1d.tikz} &
	\setlength\fheight{2.5cm} 
	\setlength\fwidth{2.5cm}
	\input{figures/integrands/sanity_hard_1d.tikz} \\ 
	\setlength\fheight{2.5cm} 
	\setlength\fwidth{2.5cm}
	\input{figures/integrands/sanity_hard_1d_exp.tikz} &
	\setlength\fheight{2.5cm} 
	\setlength\fwidth{2.5cm}
	\input{figures/integrands/two_humps_1d.tikz}
\end{tabular}
\caption{The one-dimensional integrands in our test suite.  Green lines are priors, blue lines likelihoods, and red lines are posteriors.}
\label{fig:1d_problems}
\end{figure}

\begin{figure}
\centering
\includegraphics[width=0.45\textwidth]{figures/integrands/funnel.eps}
\caption{Radford Neal's funnel problem in 2 dimensions.}
\label{fig:funnel}
\end{figure}

%\begin{minipage}[t][0.45\paperheight][t]{0.45\paperwidth}
    \input{tables/times_taken.tex}
%\end{minipage}

\input{tables/truth_prob.tex}
\input{tables/se.tex}

\begin{figure}
	\centering
	\setlength\fheight{4cm} 
	\setlength\fwidth{4cm}
	\input{figures/plots/legend.tikz}
\end{figure}

\begin{figure}
	\centering
	\setlength\fheight{4cm} 
	\setlength\fwidth{4cm}
	\input{figures/plots/se_plot_sanity_easy_1d.tikz}
\end{figure}

\begin{figure}
	\centering
	\setlength\fheight{4cm} 
	\setlength\fwidth{4cm}
	\input{figures/plots/se_plot_sanity_hard_1d.tikz} 
\end{figure}

\begin{figure}
	\centering
	\setlength\fheight{4cm} 
	\setlength\fwidth{4cm}
	\input{figures/plots/varplot_sanity_easy_1d.tikz} 
\end{figure}

\begin{figure}
	\centering
	\setlength\fheight{4cm} 
	\setlength\fwidth{4cm}
	\input{figures/plots/varplot_sanity_hard_1d_exp.tikz}
\end{figure}

\section{Conclusions}

\section*{Acknowledgements}

\bibliography{bub}
\bibliographystyle{icml2012}

\end{document} 


