%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%% ICML 2012 EXAMPLE LATEX SUBMISSION FILE %%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% Use the following line _only_ if you're still using LaTeX 2.09.
%\documentstyle[icml2012,epsf,natbib]{article}
% If you rely on Latex2e packages, like most moden people use this:
\documentclass{article}

\usepackage{preamble}

%\usepackage{subfig}
%\usepackage{hyperref}
\newcommand{\dblspace}{\setlength{\baselineskip}{0.8cm}}
\renewcommand{\pskinny}[2]{p\big(#1|#2\big)}


% For figures
\usepackage{graphicx} % more modern
%\usepackage{epsfig} % less modern
\usepackage{subfigure} 

% For citations
\usepackage{natbib}

% For algorithms
\usepackage{algorithm}
\usepackage{algorithmic}

% As of 2011, we use the hyperref package to produce hyperlinks in the
% resulting PDF.  If this breaks your system, please commend out the
% following usepackage line and replace \usepackage{icml2012} with
% \usepackage[nohyperref]{icml2012} above.
\usepackage{hyperref}

% Packages hyperref and algorithmic misbehave sometimes.  We can fix
% this with the following command.
\newcommand{\theHalgorithm}{\arabic{algorithm}}

% Employ the following version of the ``usepackage'' statement for
% submitting the draft version of the paper for review.  This will set
% the note in the first column to ``Under review.  Do not distribute.''
\usepackage{icml2012} 
% Employ this version of the ``usepackage'' statement after the paper has
% been accepted, when creating the final version.  This will set the
% note in the first column to ``Appearing in''
% \usepackage[accepted]{icml2012}


% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Submission and Formatting Instructions for ICML 2012}

\begin{document} 

\twocolumn[
\icmltitle{Sampling for Bayesian Quadrature}

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2012
% package.
\icmlauthor{Your Name}{email@yourdomain.edu}
\icmladdress{Your Fantastic Institute,
            314159 Pi St., Palo Alto, CA 94306 USA}
\icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
\icmladdress{Their Fantastic Institute,
            27182 Exp St., Toronto, ON M6H 2T1 CANADA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Bayesian Quadrature, Monte Carlor, Gaussian Processes}

\vskip 0.3in
]

\begin{abstract} 
We describe a novel approach to quadrature for probabilistic integrals, offering a competitor to traditional Monte Carlo methods. We use a Bayesian quadrature framework \citep{BZHermiteQuadrature,BZMonteCarlo}.
\end{abstract} 

\section{Introduction}

Bayesian inference often requires the evaluation of nonanalytic
definite integrals that must be approximated using numerical
integration.  In the main, these methods estimate the integral given
the value of the integrand on a set of sample points, a set that is limited in size by the computational expense of evaluating the integrand.
As discussed in \citep{MCUnsound}, traditional Monte Carlo
integration techniques do not make the best possible use of this valuable information. An alternative is found in Bayesian quadrature \citep{BZHermiteQuadrature}, that uses these samples within a Gaussian process model to perform inference about the integrand. The analytic niceties of the Gaussian then permit inference to be performed about the integral itself, the ultimate object of our interest.

\section{Gaussian Processes}

Gaussian processes (\gp s) offer a powerful method to perform Bayesian
inference about functions \citep{GPsBook}. A \gpb is defined as a
distribution over the functions $f: \Phi \rightarrow \mathbb{R}$ such
that the distribution over the possible function values on any finite
subset of $\Phi$ is multivariate Gaussian.  For a function $f(\phi)$,
the prior distribution over its values $\vf$ on a subset
$\vph \subset \Phi$ are completely specified by a mean vector
$\vmu$ and covariance matrix $K$
\begin{align*}%\label{eq:GPDefn}
\textstyle
 &\p{\vf}{I} \deq \N{\vf}{\vmu_{f}}{K_{f}}\\
 &\deq\frac{1}{\sqrt{\det{2\pi K_{f}}}}\,\exp \big(-\frac{1}{2}\,(\vf-\vmu_{f})\tra\,K_{f}\inv\,(\vf-\vmu_{f})\big),
\end{align*}
where $I$, the \emph{context}, forms the background knowledge upon which all our probabilities are conditioned. Its ubqiuity leads us to henceforth drop it from explicit representation for notational convenience. The context, $I$, includes prior knowledge of both the
mean and covariance functions, which generate $\vmu_{f}$ and
$K_{f}$ respectively. The prior mean function is chosen as
appropriate for the problem at hand (often a constant), and the
covariance function is chosen to reflect any prior knowledge about the
structure of the function of interest, for example periodicity or
differentiability. In this paper, we'll use Gaussian
covariance functions,
\begin{align} \label{eq:Gaussian_cov_fn}
% K(\vph,\vph') & \deq \prod_{e=1}^{E} K_e\big(\phi\pha_e,\phi_e'\big)\\
\textstyle
K_{f}(\phi_1,\phi_2)& \deq h_f^2\,\N{\phi_1}{\phi_2}{w_{f}}.
\end{align} 
Here $h_f$ specifies the output scale (`height') over $f$, while $w_f$ defines a (squared) input scale (`width') over $\phi$. Note that $\phi$ itself may be multi-dimensional, in which case $w_f$ must actually be a covariance matrix. Where this is true for the remainder of the paper, we'll take $w_f$ as diagonal. 
% Our \gpb distribution is specified by various hyperparameters $\theta_e\allv
% e=1,\,\ldots,\,E$, collectively denoted as $\vect{\theta} \deq
% \{\theta_e\allv e=1,\,\ldots,\, E\}$.  $\vect{\theta}$ includes the mean
% function $\vmu$, as well as parameters required by the covariance
% function, input and output scales, amplitudes, periods, etc. as
% needed.

Let us assume we have observations $(\vph_s,\vf_s)$ and
are interested in making predictions about the function
valus $f_\star$ at input $\phi_\star$. We will assume that knowledge of function inputs such as
$\vph_s$ and $\phi_\star$ is incorporated into $I$ (and will hence usually be hidden). With
this information, we have the predictive equations
$$
\pskinny{\vf_\star}{\vf_s} = 
\bN{\vf_\star}
{\meancond{f}{\vph_\star}{s}}
{\covcond{f}{\vph_\star}{s}}\,,
$$
where we have, for the mean $\novmean{a}{b}\deq \int a\,\pskinny{a}{b} \ud
a$ and variance $\novcov{a}{b}\deq \int
\big(a-\novmean{a}{b}\big)^2\,\pskinny{a}{b} \ud a$,
\begin{align} 
\textstyle
&\meancond{f}{\phi_\star}{s}
\deq \mean{f_\star}{\vf_s}
\nonumber\\
&= \mu_{f}(\phi_\star) + 
K_{f}(\phi_\star,\vph_s)
K_{f}(\vph_s,\vph_s)\inv
\bigl(\vf_s-{\mu}_{f}(\vph_s)\bigr) \label{eq:GPMean}
\\
&\covcond{f}{\phi_\star}{s}
\deq {\cov{f_\star}{\vf_s}} 
\nonumber\\
&= K_{f}(\phi_\star,\phi_\star) - 
K_{f}(\phi_\star,\vph_s)
K_{f}(\vph_s,\vph_s)\inv
K_{f}(\vph_s,\phi_\star)\,. \nonumber%\label{eq:GPCov}
\end{align} 

\section{Bayesian Quadrature} \label{sec:BQ}

% Note that maximum likelihood is also subject to issues. $\p{D}{\phi,I}$, how come (known) $I$ is on the right and (known) $D$ is on the left? 

\emph{Bayesian quadrature} \citep{BZHermiteQuadrature,BZMonteCarlo} is a means of
performing Bayesian inference about the value of a potentially
nonanalytic integral 
\begin{equation} \label{eq:intyf}
 \inty{f} \deq \int f(\phi)\,\po{\phi}\,\ud\phi\,.
\end{equation}
Note that we use a condensed notation; this and all integrals to follow are definite integrals over the entire domain of interest.
We'll assume we are integrating with respect to a Gaussian
prior
\begin{align}\label{eq:phiprior}
\textstyle
 \po{\phi} & \deq \N{\phi}{\nu_{\phi}}{\lambda_{\phi}} \,,
\end{align}
although other convenient forms, or, if necessary, the use of an
importance re-weighting trick, allow any other integral to be
approximated \citep{OsborneAnon}. If $\phi$ is a vector, $\nu_{\phi}$ is a  vector of identical size, and $\lambda_{\phi}$ an appropriate covariance matrix.

Quadrature involves evaluating $f(\phi)$ at a
vector of sample points $\vph_s$, giving $\vf\pha_s\deq
f(\vph_s)$. Of course, this evaluation is usually a computationally expensive
operation.
%---we clearly can't afford to evaluate $f(\phi)$ for all possible inputs $\phi$ for an unbounded domain $\Phi$. 
The resultant sparsity of our samples introduces uncertainty about the function $f$ between them, and hence uncertainty about the integral $\inty{f}$.

As ever in the face of uncertainty, we address the estimation of the value of our integral as a problem of Bayesian inference \citep{BZNumericalAnalysis}. In considering any problem of inference, we need to be clear about both what information we have and which uncertain variables we are interested in. In our case, both the values $f(\vphS)$ and their locations $\vphS$ represent valuable pieces of knowledge. As discussed by \citet{MCUnsound}, traditional Monte Carlo, which approximates as
\begin{equation} \label{eq:MC_integral_estimate}
\inty{f} \simeq \frac{1}{\card{s}} \sum_{i=1}^{\card{s}} f(\phi_i)\,,
\end{equation}
effectively ignores the information content of $\vphS$, leading to unsatisfactory behaviour\footnote{
  For example, imagine that we had $\card{s}=3$, and $\phi_1 = \phi_2$. In this case, the identical value $q(\phi_1)= q(\phi_2)$ will receive $\nicefrac{2}{3}$ of the weight, whereas the equally useful $q(\phi_3)$ will receive only $\nicefrac{1}{3}$.}.

% As with our convention above, we will take knowledge of
% sample locations $\vph_s$ to be implicit within $I$. However, as we don't know $f(\phi)$ for any $\phi \not \in \vphS$, we are uncertain about the function $f(\cdot)$. As a consequence, we are also uncertain about the value of the integral $\inty{f}$. As such, we possess probability distributions over both $f(\cdot)$ and $\inty{f}$. 
% %The resulting Bayesian network is depicted in Figure \ref{fig:BMC}.

%  \begin{figure}[ht]
% \hspace{-1cm}
% 	\begin{pspicture}(-5,0)(5,4.25)%
% 	%\showgrid
% 	\GM@Inode{0}{3.5}{1}%	
% 	%\rput(I){\rput(0,-2){\GM@node{X}}}   \GM@label[angle=90]{X}{$X$}
% 	\rput(0,2){\GM@detnode{psi}}   \GM@label[angle=-90]{psi}{$\rv{\inty{f}}$}
% 
% % NB \Phi is the actual value of the hyperparameters -- doesn't make any sense to write \Phi_i
% 
% 	\rput(psi){\rput(1.25,-2){\GM@plate[plateLabelPos=bl]{2}{4.2}{$i'\not\in s$}}}
% 	\rput(psi){\rput(2.5,1){\GM@node[observed=true]{phij}}}   \GM@label[angle=90]{phij}{$\rv{\phi}_{i'}$}
% 	\rput(phij){\rput(0,-2){\GM@detnode{qj}}}   \GM@label[angle=130]{qj}{$\rv{f}_{i'}$}
% 
% 	\rput(psi){\rput(-3.25,-2){\GM@plate[plateLabelPos=br]{2}{4.2}{$i \in s$}}}
% 	\rput(psi){\rput(-2.5,1){\GM@node[observed=true]{phii}}}   \GM@label[angle=90]{phii}{$\rv{\phi}_i$}
% 	\rput(phii){\rput(0,-2){\GM@detnode[observed=true]{qi}}}   \GM@label[angle=50]{qi}{$\rv{f}_i$}
% 
% 	\pnode(0,0.5){mid}
% 
% 	%\ncline[arrows=->]{phi}{X}
% 
% 
% 	\ncline[arrows=->]{phij}{qj}
% 	\ncline[arrows=->]{qj}{psi}
% 	\ncline[arrows=->]{phij}{psi}
% 
% 	\ncline[arrows=->]{phii}{qi}
% 	\ncline[arrows=->]{qi}{psi}
% 	\ncline[arrows=->]{phii}{psi}
% 
% 	\ncarc{qj}{qi}
% 	\nccircle[angleA=-90]{qj}{0.5}
% 	\nccircle[angleA=90]{qi}{0.5}
% 
% 	\end{pspicture}%
% \caption{Bayesian network for Bayesian Quadrature.}
% \label{fig:BMC}
% \end{figure}

We choose for $f$ a 
\gpb prior with mean $\mu_f$ and the Gaussian covariance function \eqref{eq:Gaussian_cov_fn}.
Here the scales $h_f$ and $w_f$ are \emph{quadrature hyperparameters}, hyperparameters that specify the
 \gpb used for Bayesian quadrature. These scales, and the others that follow, will be taken as given and incorporated into the (hidden) context $I$.

% Many more of these will be implicitly introduced in the coming sections; we'll take this as given and incorporate them into the (hidden) context $I$. 
% Note that it will later become apparent that our inference for $\inty{f}$ is independent of the $h_f$ quadrature hyperparameter.

Note that variables over which we have a multivariate Gaussian distribution are jointly Gaussian distributed with any projections of those variables. Because integration is a projection, we can hence use our computed samples $\vf_s$ to perform analytic Gaussian process inference about the value of integrals over $f(\phi)$, such as $\inty{f}$. Our mean estimate for $\inty{f}$ given $\vf_s$ is

\begin{align} \label{eq:mean_inty_f}
\mean{\inty{f}}{\vf_s}
& 
=\iint \inty{f}\,\p{\inty{f}}{f}\p{f}{\vf_s} \ud \inty{f} \,\ud f                                                                                                                                                               \nonumber\\
&
 =\iint \inty{f}\,\dd{\inty{f}}{\int f(\phi)\,\po{\phi}\,\ud\phi}
\nonumber\\
&\hspace{2.5cm}
\N{f}{\meancondfn{f}{s}}{\covcondfn{f}{s}} \ud \inty{f} \,\ud f \nonumber\\
&
 = \int \meancondfn{f}{s}(\phi)\,\po{\phi}\,\ud\phi\nonumber\\
&
 = 
%\N{\inty{f}}
\mu_f + \ntT{s}{f}\, \dtt{s}{f}
%{\varpi_{f}-\ntT{s}{f} K_{f}(\vph_s,\vph_s)\inv \nt{s}{f}}
\,,
\end{align}
where for $\phi_i \in \vph_s$,
\begin{align*}
\fnt{\phi_i}{f} & \deq \!\int K_f(\phi_i,\phi)\po{\phi}\ud\phi
 = h_f^2\,\N{\phi_i}{\nu_{\phi}}{\lambda_{\phi}+w_{f}}\\
%\intertext{and}
\dtt{s}{f} & \deq K_{f}\bigl(\vph_s,\vph_s\bigr)\inv (\vf_s-{\mu}_{f})\,.
\end{align*}
Note that the form of our `best estimate' for $\inty{f}$, \eqref{eq:mean_inty_f}, is an affine combination of the samples $\vf_s$, just as for traditional quadrature or Monte Carlo techniques. Indeed, if $\mu_f$ is taken as the mean of $\vf_s$ (as is usual for \gpb inference), the second term in \eqref{eq:mean_inty_f} can be viewed as a correction factor to the Monte Carlo estimate \eqref{eq:MC_integral_estimate}.
Note also that $h_f$ represents a simple multiplicative factor to both $\ntT{s}{f}$ and $K_{f}\bigl(\vph_s,\vph_s\bigr)$, and as such cancels out of \eqref{eq:mean_inty_f}. 

For our later use, consider now the more general integral over the product of two functions, $f$ and $g$,
$$\inty{f g} \deq \int f(\phi)\,g(\phi)\,\po{\phi}\,\ud\phi\,,$$ 
for which, given independent \gp s (with constant mean and Gaussian covariance) and function observations $\vf_{s} \deq f(\vph_{s})$ and  $\vg_{t} \deq g(\vph_{t})$, we have
% \begin{align} \label{eq:mean_inty_fg}
% \mean{\inty{f g}}{\vf_s,\,\vect{g}_t}
% & 
% =\iint \inty{f g}\,\p{\inty{f g}}{f,\,g}\p{f}{\vf_s}\,\p{g}{\vect{g}_t}\, \ud \inty{f g} \,\ud f \,\ud g                                                                                                                                                                 \nonumber\\
% &
% %  =\iint \inty{f g}\,\dd{\inty{f g}}{\int f(\phi)\,g(\phi)\,\po{\phi}\,\ud\phi}\N{f}{\meancondfn{f}{s}}{\covcondfn{f}{s}}\,\N{g}{\meancondfn{g}{s}}{\covcondfn{g}{s}}\, \ud \inty{f g} \,\ud f\,\ud g \nonumber\\
% % &
%  = 
% %\N{\inty{f}}
% {\dtt{s}{f}}\tra \,\Nt_{f,g}(\vph_s,\vph_t)\,\dtt{t}{g}
% %{\varpi_{f}-\ntT{s}{f} K_{f}(\vph_s,\vph_s)\inv \nt{s}{f}}
% \,,
% \end{align}
% where, for $\phi_i \in \vph_s,\,\phi_j \in \vph_t$,
% \begin{align*} %\label{eq:Nt}
% \Nt_{f,g}(\phi_i,\phi_j) & \deq 
% \int K_f(\phi_i,\phi)\,\po{\phi}K_g(\phi,\phi_j)\, \ud\phi\nonumber\\
% & =h_f^2\, h_g^2\,
% \N{\begin{bmatrix} \phi_i \\ \phi_j \end{bmatrix}}{\begin{bmatrix} \nu_{\phi}\\ \nu_{\phi} \end{bmatrix}}{\begin{bmatrix}  \lambda_\phi+W_f & \lambda_\phi \\ \lambda_\phi & \lambda_\phi+W_g
% \end{bmatrix}}\,.
% \end{align*}
% Again, the scales $h_f$ and $h_g$ cancel out of our mean estimate, in this case, \eqref{eq:mean_inty_fg}.
% 
% Finally, consider the  integral over three independent functions, $f$, $g$ and $h$,
% $$\inty{f g h} \deq \int f(\phi)\,g(\phi)\,h(\phi)\,\po{\phi}\,\ud\phi\,,$$ 
\begin{align*} %\label{eq:mean_inty_fgh}
& \mean{\inty{f g}}{\vf_{s},\,\vg_{t}} \nonumber\\
& 
=
\iint \inty{f\,g}\,\p{\inty{f\,g}}{{f\,g}}
% \nonumber\\
% &
%\hspace{3cm}
\p{f}{\vf_s}\p{g}{\vg_t}\ud \inty{f\,g}\ud f\ud g                                                                                                                                                                      \nonumber\\
&
%  =\iint \inty{f g}\,\dd{\inty{f g}}{\int f(\phi)\,g(\phi)\,\po{\phi}\,\ud\phi}\N{f}{\meancondfn{f}{s}}{\covcondfn{f}{s}}\,\N{g}{\meancondfn{g}{s}}{\covcondfn{g}{s}}\, \ud \inty{f g} \,\ud f\,\ud g \nonumber\\
% &
 = 
%\N{\inty{f}}
\mu_f\,\mu_g
+ \mu_f\,\ntT{s}{g}\, \dtt{s}{g}
+ \mu_g\,\ntT{s}{f}\, \dtt{s}{f}
\nonumber\\
&\hspace{3cm}
+ \dtt{{s}}{f}\tra\,\Ot_{f,g}\bigr(\vph_{s},\,\vph_{t}\bigr)\,\dtt{{t}}{g}
%{\varpi_{f}-\ntT{s}{f} K_{f}(\vph_s,\vph_s)\inv \nt{s}{f}}
\,,
\end{align*}
where, for $\phi_i \in \vph_{s}$ and $\phi_j \in \vph_{t}$,
\begin{align*} %\label{eq:Nt}
& \Ot_{f,g}\bigr(\phi_{i},\,\phi_{j}\bigl) 
\deq 
\!\int K_{f}(\phi_i,\phi)\,K_{g}(\phi_j,\phi)\,\po{\phi}\, \ud\phi\nonumber\\
& =h_{f}^2\, h_{g}^2\,
\N{\begin{bmatrix} \phi_i \\ \phi_j \end{bmatrix}}
{\begin{bmatrix} \nu_{\phi}\\ \nu_{\phi}\end{bmatrix}}
{\begin{bmatrix}  
\lambda_\phi+w_{f} & \lambda_\phi \\ 
\lambda_\phi & \lambda_\phi +w_{g}
\end{bmatrix}}\,.
\end{align*}
Here we have overloaded the definition of $\Ot$; its two definitions can be clearly distinguished by the number of arguments with which it is presented. 
Again, the scales $h_f$ and $h_g$ cancel out of our mean estimate.
%, in this case, \eqref{eq:mean_inty_fg}.

\section{Linearisation}

Imagine we want to evaluate the evidence $\inty{r}$, an integral over non-negative likelihoods, $r(\phi)$, who arguments are the parameters of the relevant model.
$$
\inty{r} = \int r(\phi) p(\phi) \ud\phi
$$

Firstly, we define the transformation
\begin{align*}
t\bigl(r(\phi)\bigr) & \deq \log\left(\nicefrac{r(\phi)}{\gamma} + 1\right)
% \intertext{and its inverse}
% t\inv\bigl(\tr(\phi)\bigr) & \deq \gamma \bigl(\exp \tr(\phi)-1\bigr)
\end{align*}
% and their inverses
% $$
% t_q\inv\bigl(\tq(\phi)\bigr) \deq \gamma\bigl(\eta+\exp \tq(\phi)\bigr)
% \qquad \text{and} \qquad
% t_q\inv\bigl(\tq(\phi)\bigr) \deq \gamma\bigl(\eta+\exp \tq(\phi)\bigr)\,,
% $$
(which has well-defined inverse $t\inv$) where 
$
 \gamma \deq 100\, \max(\vr_s)
$
is a constant introduced for superior numerical accuracy.

We now assign \gpb priors to the functions
\begin{align*}
 \tr & \deq  t(r)
\end{align*}
% We assume that far away from our existing observations,
% the means for $q$ and $r$ are equal to a value suitably close to
% zero, and hence take a corresponding zero prior mean for the \gp s over $\tq$ and $\tr$. 
with Gaussian
covariances of the form \eqref{eq:Gaussian_cov_fn}.
%the minimum of the existing observations.
These choices of prior distribution are motivated by the fact that
$l$ is strictly positive and possesses a large dynamic
range. 


\begin{align}
\mean{\inty{r}}{\vr_s}
% & 
% =\iint \inty{l}\,\p{\inty{l}}{l}\p{l}{\vl_s} \ud \inty{l} \,\ud l                                                                                                                                                               \nonumber\\
% &
&  =\int \Bigl( \int t\inv\bigr(\tr(\phi)\bigl)\,\po{\phi}\,\ud\phi\Bigr)
\N{\tr}{\meancondfn{\tr}{s}}{\covcondfn{\tr}{s}} \ud \tr \nonumber
\end{align}

Here the exponential in $t\inv$ ruins the linearity that yielded the analyticity in section \ref{sec:BQ}. To combat this, we perform inference for $\inty{r}$ directly as a functional of $\tr$.

\begin{align*}
 \psi[\tr] \deq \inty{r} & = \gamma\Bigl(\int  \exp \tr(\phi) p(\phi) \ud \phi-1\Bigr)\\
\pderiv{}{\tr(\phi)}\psi[\tr] & = \gamma \exp \tr(\phi) p(\phi)
\end{align*}

We use another \gpb for $\psi$, with the affine covariance
\begin{equation*}
 K_\psi(\tr,\tr')
% \deq 
%  K_\psi\bigl((\tvq\pha_c,\tvr\pha_c),(\tvq'_c,\tvr'_c)\bigr)
\deq
\int\tr(\phi) \tr'(\phi) \ud \phi
+ \omega^2
\end{equation*}
reflecting the assumption that $\psi$ is, as desired, affine in $\tr$. With this covariance, and given observations
$\psi_0 \deq \psi(\tr_0)$, $\pderiv{\psi_0}{\tr(\phi)} \deq \pderiv{\psi}{\tr(\phi)}(\tr_0)$ (whose values we'll return to later), we have the \emph{linearisation} approximation
\begin{multline}\label{eq:linearisation}
\novmean{\psi[\tr]}{\psi_0,\pderiv{\psi_0}{\tq(\phi)}, \tr} 
= \psi_0+\\
\int \pderiv{\psi_0}{\tr(\phi)}\bigl(\tr(\phi)-\tr_0(\phi)\bigr)\ud\phi\,.
\end{multline}

We are going to perform this linearisation of $\psi(\tq,\tr)$ around the point defined by $\tr_0 \deq t(m_{r|s})$. $m_{r|s}$ is the \gpb conditional mean (as per \eqref{eq:GPMean}) for $r$ given observations $r(\vph_s)$. For this \gpb (over the non-log space), we take zero prior means and Gaussian
covariances of the form \eqref{eq:Gaussian_cov_fn}. With appropriate selections of input scales $w_r$ for such covariances, and defining
\begin{align*}
\Delta & \deq m_{\tilde{r}|s} - t(m_{r|s})\,,
\end{align*}
(the differences between the \gpb means over our transformed quantities and the transformed \gpb means over original quantities) we have
\begin{align}\label{eq:mt_sim_tm}
\Delta \simeq 0\,,
\end{align}
That is, $\tr_0$ is close to the peaks of our Gaussians over $\tr$, rendering our linearisation appropriate. The choice of this point is motivated by the fact that for it, our integrals become tractable.

Our linearisation corresponds to giving our \gpb over $\psi$ observations at $\tr_0=t(m_{r|s})$ of both the functional itself,
\begin{align}
\psi_0 & \deq \psi[\tr_0]
= 
{\mean{\inty{r}}{\vr_s}} \label{eq:rho_0}
\intertext{along with its functional derivatives}
\pderiv{\psi_0}{\tr(\phi)} & \deq \pderiv{\psi}{\tr(\phi)}[\tr_0]
 = \bigl(\mean{r(\phi)}{\vr_s}+\gamma\bigr)\,\po{\phi}
\nonumber
\end{align}
As with any linearisation approximation, \eqref{eq:linearisation} gives the value at the selected point $\tr_0$, plus a correction factor modelling the influence of the first derivatives. 
These correction factor contains a further non-analytic integral, over $\phi$. Fortunately, they're not too far away from being analytic; almost all terms with dependence on $\phi$ within those integrals are Gaussian. The exception is the $t_r(m_{r|s})$ terms within $\Delta$. As such, we perform another stage of Bayesian quadrature by treating $\Delta$ as an unknown function of $\phi$.
For this function we take Gaussian process priors with zero prior mean and Gaussian covariance \eqref{eq:Gaussian_cov_fn}. We must now choose sample points $\vph_c$ at which to evaluate our $\Delta$ function. 
% Unfortunately, it is impractical to resolve this decision problem in the same manner as the selection of $\vphS$, our ultimate goal (a topic that receives our full attention in Section \ref{sec:SBQ})).
$\vph_c$ should firstly include $\vph_s$, at which points we know that $\delta$ is equal to zero. Note firstly that a simple heuristic for determining the `best' samples (in the sense of samples with which to fit a \gp) is to select those samples at extrema. Note that the peaks and troughs of $\delta$ occur at points far-removed from $\vph_s$, but no further away than a few input scales. This is where the transformed mean for $r$ is liable to differ most from the mean of the transformed variable (and the appropriate $\Delta$ extremised). That is, we would like to select $\vph_c$ as points as far away from all points in $\vph_s$ as possible, while still remaining within a few input scales. 
%For convenience, we define the allowable region to be the box defined by the least upper bound of $\vph_s$ plus three input scales and the greatest lower bound minus three input scales. 
Finding $\vph_c$ hence requires solving the largest empty sphere problem, which we can solve with the use of a Voronoi diagram \citep{Voronoi, shamos1975closest, okabe1997locational}. 
Where this requires computation in excess of that afforded by our allowance, we instead construct a \acro{kd}-tree \citep{bentley1975multidimensional} for our $\vph_s$, and select $\vph_c$ as the centres of the hyper-rectangles defined by the splitting
hyperplanes of the tree.

Note that our choice of $\tr_0$ allowed us to resolve the integrals required. For future notational convenience, we assume that if we have an observation of $\psi_0$, we will also have observations of its functional derivatives.

\begin{align}
& \mean{\inty{r}}{\psi_0,\tvr_s} \nonumber\\
& \deq \int \mean{\psi[\tr]}{\psi_0,\tr}
\p{\tr}{\tvr_s}\, \ud \tr 
\nonumber\\
& = \mean{\psi[\tr]}{\psi_0,m_{\tr|s}} \nonumber\\
& = \mean{\inty{r}}{\vr_s} + \iint \bigl(\mean{r(\phi)}{\vr_s}+\gamma\bigr)\,\Delta(\phi)\,\po{\phi}\ud\phi
\nonumber\\
& = \mean{\inty{r}}{\vr_s} + \mean{\inty{r \Delta}}{\vr_s} + \gamma\, \mean{\inty{ \Delta}}{\vr_s}
\label{eq:mean_ev}
\end{align}

\section{Sampling for Bayesian Quadrature}

We aim to select samples $\phi_s$ so as to minimise the uncertainty in the evidence. We expect such samples to be useful not just for estimating the evidence, but also for any other related expectations, such as would be required to perform prediction using the model.

The variance in the evidence is
\begin{align*}
\cov{\langle r\rangle}{\tvr_{s}}
& = \int \inty{r}^2 \p{\inty{r}}{\tvr_s} \ud\inty{r}- \mean{\inty{r}}{\tvr_s}^2\,.
\end{align*}
To resolve the first term, define a new functional
\begin{align*}
 \chi[\tr] \deq \inty{r}^2 & = \gamma^2\bigl(\int  \exp \tr(\phi) p(\phi) \ud \phi-1\bigr)^2\\
\pderiv{}{\tr(\phi)}\chi[\tr] & = 2\gamma \exp \tr(\phi) p(\phi) \bigl(\int  \exp \tr(\phi) p(\phi) \ud \phi-1\bigr)
\end{align*}
which we again linearise around $\tr_0$. Hence, as per the above
\begin{align*}
& \cov{\langle r\rangle}{\chi_0,\psi_0,\tvr_{s}} = \mean{\inty{r}^2}{\chi_0,\tvr_s} - \mean{\inty{r}}{\psi_0,\tvr_s}^2
% & =  (\mean{\inty{r}}{\vr_s})^2 \\
% & \qquad + 2\mean{\inty{r}}{\vr_s}
% \bigl(\mean{\inty{r \Delta}}{\vr_s} + \gamma\, \mean{\inty{\Delta}}{\vr_s}\bigr)
% \\
% & \qquad - \bigl(
% \mean{\inty{r}}{\vr_s} + \mean{\inty{r \Delta}}{\vr_s} + \gamma\, \mean{\inty{ \Delta}}{\vr_s}
% \bigr)^2
% & = \mean{\inty{r}}{\vr_s}
% \bigl(\mean{\inty{r \Delta}}{\vr_s} + \gamma\, \mean{\inty{\Delta}}{\vr_s}\bigr)
% \\
% & \qquad - \bigl( \mean{\inty{r \Delta}}{\vr_s} + \gamma\, \mean{\inty{ \Delta}}{\vr_s}
% \bigr)^2
% \,.
\end{align*}
where
\begin{align*}
&\mean{\inty{r}^2}{\chi_0,\tvr_s} \\
& = \mean{\chi[\tr]}{\chi_0,m_{\tr|s}} \\
& = (\mean{\inty{r}}{\vr_s})^2 \\
& \hspace{1cm} + 2\mean{\inty{r}}{\vr_s}
\bigl(\mean{\inty{r \Delta}}{\vr_s} + \gamma\, \mean{\inty{\Delta}}{\vr_s}\bigr)
\end{align*}


Consider adding a new sample at $\phi_a$. The expected variance in our evidence after doing so is
\begin{align}
& \bigl\langle \cov{\langle r\rangle}{\chi_0,\psi_0,\tvr_{s,a}}\mid \chi_0,\psi_0,\tvr_{s}\bigr\rangle\nonumber\\
& = \mean{\inty{r}^2}{\chi_0,\tvr_s}  - 
\int\mean{\inty{r}}{\psi_0,\tvr_{a,s}}^2\p{\tr_a}{\tvr_s}\ud\tr_a\,.\label{eqn:exp_var}
\end{align}
The first term is independent of the selection of $\phi_a$ and hence can be safely ignored. Noting that $\int \exp(c\, \tr_a)\, \N{\tr_a;m, \sigma^2} \ud\tr_a = \exp(c\, m + \nicefrac{1}{2}\, c^2 \sigma^2)$, the second term can be resolved analytically for any trial $\phi_a$ (we omit the laborious details of doing so). With this expression, we now select the $\phi_a$ that minimises the second term using a numerical optimisation technique. 

\section{Marginalising quadrature hyperparameters}

Hitherto, we have taken the maximum likelihood values $m_\theta$ for the quadrature hyperparameters $\theta$ of the quadrature-\gpb used to model the integrand. Once we have very many samples, this approximation seems reasonable: with lots of data, we can be almost certain about the values of the quadrature hyperparameters. However, in the process of taking samples, we'd like to acknowledge the influence new samples may have on our beliefs about the quadrature-hyperparameters. We'd expect this would further promote exploration, particularly early on, so as to firm up our beliefs about the quadrature-hyperparameters. The quadrature-hyperparameters of most interest are the input scales $w_r$; these hyperparameters can have a very dramatic influence on our fit to a function.

Unfortunately, the dependence of our predictions upon these input scales is complex, ruling out analytic results. However, any approximation we make is likely to improve upon our existing assumption that the posterior for all quadrature-hyperparameters is a delta function! Our approach will be to assume that
\begin{itemize}
 \item that the posterior for $\theta$ is Gaussian with mean equal to the maximum likelihood value $m_\theta$, essentially assuming that our prior is very broad. 
\item The Gaussian's covariance matrix $C_\theta$ is taken as diagonal. For quadrature hyperparameters other than the input scales, we take the appropriate diagonal elements as infinitesimally small, such that our posterior for these parameters is a delta function. For the the diagonal elements corresponding to $w_r$, we use the squared input scales $w_s$ of another \gp, fitted to the likelihood $s(w_r)$ of the input scales $w_r$. 
\item that our predictions for $\tr_a$ have a mean which is linear in $w_r$ around the maximum likelihood values, and a variance which is essentially constant.  
\end{itemize}
 
With these assumptions, we have
\begin{align*}
& \mean{\inty{r}}{\psi_0,\tvr_s} \\
& \deq \iint \mean{\psi[\tr]}{\psi_0,\tr}
\p{\tr}{\tvr_s, \theta}\, \p{\theta}{\tvr_s}\ud \tr\,\ud \theta\\
& = \iint \mean{\psi[\tr]}{\psi_0,\tr} \N{\tr}{m_{\tr|s,\theta}}{C_{\tr|s,\theta}}
\\
& \hspace{5cm}\N{\theta}{m_\theta}{C_\theta}\ud \tr\,\ud \theta
\\
& \simeq \iint \mean{\psi[\tr]}{\psi_0,\tr} 
\\
& \hspace{2cm}\N{\tr}
{m_{\tr|s,m_\theta}+\pderiv{m_{\tr|s,\theta}}{\theta}(\theta-m_\theta)}
{C_{\tr|s,m_\theta}}
\\
& \hspace{4cm}
\N{\theta}{m_\theta}{C_\theta}\ud \tr\,\ud \theta
\\
& = \iint \mean{\psi[\tr]}{\psi_0,\tr} \\
& \hspace{1cm}\N{\tr}
{m_{\tr|s,m_\theta}}
{C_{\tr|s,m_\theta}+\pderiv{m_{\tr|s,\theta}}{\theta}C_\theta\pderiv{m\tra_{\tr|s,\theta}}{\theta}}
\ud \tr
\\
& = \mean{\psi[\tr]}{\psi_0,m_{\tr|s,m_\theta}}\,,
\end{align*}
which is identical to \eqref{eq:mean_ev} (note that we had previously just implicitly assumed that $\theta=m_\theta$). However, for the expected variance after adding a sample, we have
\begin{align}\label{eqn:exp_var_wtheta}
& \bigl\langle \cov{\langle r\rangle}{\chi_0,\psi_0,\tvr_{s,a}}\mid \chi_0,\psi_0,\tvr_{s}\bigr\rangle
\nonumber\\
& =  \mean{\chi[\tr]}{\chi_0,m_{\tr|s,m_\theta}}  - 
\int\mean{\inty{r}}{\psi_0,\tvr_{a,s}}^2
\nonumber\\
& \hspace{2cm}
\times\N{\tr_a}
{\tilde{m}_a }
{\tilde{C}_a +\pderiv{\tilde{m}_a}{\theta}C_\theta\pderiv{\tilde{m}\tra_a}{\theta}}
\ud\tr_a\,.
\end{align}
where
\begin{align*}
\tilde{m}_a & \deq \mean{\tr_a}{\tvr_s,m_\theta}\\
\tilde{C}_a & \deq \cov{\tr_a}{\tvr_s,m_\theta}\,.
\end{align*}
As with \eqref{eqn:exp_var}, \eqref{eqn:exp_var_wtheta} can be evaluated analytically, and minimised to determine the optimal $\phi_a$.
Hence our variance for $\tr_a$ has been inflated due to our consideration of the influence of hyperparameters, which we expect to promote additional exploration, as desired.


\bibliography{bub}
\bibliographystyle{icml2012}

\end{document} 


% This document was modified from the file originally made available by
% Pat Langley and Andrea Danyluk for ICML-2K. This version was
% created by Lise Getoor and Tobias Scheffer, it was slightly modified  
% from the 2010 version by Thorsten Joachims & Johannes Fuernkranz, 
% slightly modified from the 2009 version by Kiri Wagstaff and 
% Sam Roweis's 2008 version, which is slightly modified from 
% Prasad Tadepalli's 2007 version which is a lightly 
% changed version of the previous year's version by Andrew Moore, 
% which was in turn edited from those of Kristian Kersting and 
% Codrina Lauth. Alex Smola contributed to the algorithmic style files.  


