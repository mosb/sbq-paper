\documentclass{article}
\usepackage{preamble}
%\usepackage{subfig}
\newcommand{\dblspace}{\setlength{\baselineskip}{0.8cm}}
\renewcommand{\pskinny}[2]{p\big(#1|#2\big)}
\usepackage{graphicx} % For figures
\usepackage{subfigure} 
\usepackage{natbib}   % For citations
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{hyperref}
\newcommand{\theHalgorithm}{\arabic{algorithm}}
\usepackage[normalem]{ulem}  % for strikethrough
\usepackage{color} % for comments to each other
\usepackage{comment}
\usepackage{pgfplots}
\usepackage{icml2012}

% The width of one column in cm: \printinunitsof{cm}\prntlen{\columnwidth} = 8.25381

% \usepackage[accepted]{icml2012}ยก

% The \icmltitle you define below is probably too long as a header.
% Therefore, a short form for the running title is supplied here:
\icmltitlerunning{Sampling for Bayesian Quadrature}

\begin{document} 
\twocolumn[
\icmltitle{
Doubly-Bayesian Quadrature for Active Learning of Model Evidence}
%Sampling for Bayesian Quadrature\\or\\Actively Learning Normalization Constants\\or\\Doubly Bayesian Quadrature: The \acro{bbq} Algorithm\\or\\

% It is OKAY to include author information, even for blind
% submissions: the style file will automatically remove it for you
% unless you've provided the [accepted] option to the icml2012
% package.
\icmlauthor{Your Name}{email@yourdomain.edu}
\icmladdress{Your Fantastic Institute,
            314159 Pi St., Palo Alto, CA 94306 USA}
\icmlauthor{Your CoAuthor's Name}{email@coauthordomain.edu}
\icmladdress{Their Fantastic Institute,
            27182 Exp St., Toronto, ON M6H 2T1 CANADA}

% You may provide any keywords that you 
% find helpful for describing your paper; these are used to populate 
% the "keywords" metadata in the PDF but will not be shown in the document
\icmlkeywords{Bayesian Quadrature, Monte Carlo, Gaussian Processes, Numerical Integration, Model Evidence, Likelihood ratios}

\vskip 0.3in
]

\begin{abstract} 
%We describe a novel approach to quadrature for probabilistic integrals, offering a competitor to traditional Monte Carlo methods. We use a Bayesian quadrature framework \citep{BZHermiteQuadrature,BZMonteCarlo}.
We introduce several innovations making Bayesian Quadrature methods suitable for computing model evidences, or normalization constants.  We demonstrate the many advantages of model-based integration over standard Markov-chain Monte Carlo approaches.  These include sample efficiency, a natural stopping criterion and an estimate of uncertainty in our integral. We extend existing Bayesian quadrature approaches to this problem by modeling the non-negativity of our integrand and approximately marginalising over the hyperparameters of our Gaussian process model. We also propose the use of active learning to select our samples as an alternative to Monte Carlo methods.
\end{abstract} 

\section{Introduction}

Training or evaluating any probabilistic model typically requires an integration over model parameters, weighted by their likelihoods.  This common problem has many names:  computing the model evidence, estimating the partition function or normalizing a distribution.  Typically, this task is performed using Markov-Chain Monte Carlo (\acro{mcmc}) methods.  These methods have many well-known problems, such as requiring extensive tuning, becoming stuck in local modes, and falsely appearing to converge \citep{NealMC}.  However, almost all standard approaches fall into the wide family of \acro{mcmc} methods.

These randomized methods estimate the model evidence $Z$ given the value of the integrand on a set of sample points.  The number of points sampled is limited in size by the computational expense of evaluating the integrand. As discussed in \citep{MCUnsound}, traditional Monte Carlo integration techniques do not make the best possible use of this valuable information. An alternative is found in Bayesian quadrature (\acro{bq}) \citep{BZHermiteQuadrature}, a method which uses function samples within a Gaussian process model to compute a closed-formed posterior over the value of the integral.

Bayesian Quadrature has previously been used to infer model evidence, \citep{BZMonteCarlo}.  However the approach used there suffered from three difficulties which are overcome in this paper: 

\paragraph*{Log-prior:} Previous work used a \gpb prior inappropriate for likelihood functions, which are strictly positive and have a high dynamic range.  We introduce tractable approximate inference under a log-\gp.

\paragraph*{Hyperparameter Marginalisation:} Uncertainty in the quadrature hyperparameters has previously been ignored, leading to pathologies.  We introduce a tractable approximation which accounts for uncertainty in hyperparameter values; in contrast to the hand-tuning required for \acro{mcmc} methods.

\paragraph*{Active Sampling:} In \citep{BZMonteCarlo}, sample points were chosen using a Markov Chain.  We demonstrate how to actively choose function samples in order to minimize the uncertainty in our integral.

As well, we compare our \acro{bq} approach to standard \acro{mcmc} techniques on a real scientific problem, where we demonstrate one of the key advantages of \acro{bq}: a closed-form expression for the posterior uncertainty in our integral provides a natural stopping criterion for approximate inference.

\section{Estimating Model Evidence via \acro{mcmc}}

%Here, we give a brief overview of the standard methods used for computing normalization constants.  For the remainder of the paper, we will assume that we are able to draw samples from 

Simple Monte Carlo (\acro{smc}) generates samples $x_1 \dots x_N$ from the prior distribution.  Then we estimate $Z$ by $$\hat{Z} = \frac{1}{N} \sum_{n=1}^{N} \lfn(x_n).$$  An estimate of the variance of $\hat{Z}$ is given by the standard error of $\lfn({\bf x})$.

%\paragraph*{Annealed Importance Sampling} \citep{neal2001annealed} is a popular thermodynamic integration method which 

There exist several other standard methods for computing partition functions, such as annealed importance sampling (\acro{ais}) \citep{neal2001annealed}, nested sampling \citep{skilling2004nested} and bridge sampling.  For a review, see \citet{chen2000monte}.

One of the main difficulties with using \acro{mcmc} methods in practice is that most methods have at least a handful of parameters which are typically set by hand. 

Convergence diagnostics exist to indicate if a Markov chain is failing to 'mix', or explore the entire integrand.  However, these methods are known to have failure modes in which the chain does not mix, however the diagnostics fail to indicate there is a problem \citep{NealMC} [need more citations].

An alternative approach to evaluating $Z$ is to posit a distribution over likelihood functions, and update that distribution based on information observed about the likelihood function.

 \begin{figure}
 \centering
 \psfragfig[width=\columnwidth]{figures/bmc_intro}
 \caption{An illustration of model-based integration, or Bayesian Quadrature.  We compute the expected are underneath Gaussian process posterior, conditioned on sampled values of the true likelihood function.  Obtaining samples about uncertain regions allows us to reduce our uncertainty about the area under the function.  }
 \label{fig:model_based}
 \end{figure}

Having a reliable indicator that our estimate has converged to the truth is vital.  Using a model-based approach, the issue of convergence is solved naturally:  our posterior variance over $Z$ indicates our remaining uncertainty about the quantity we care about.  This statistic is typically only reliable if we are able to integrate over all unknown quantities.  This is typically intractable, however by approximately integrating out lengthscales in Section \ref{sec:marginalization} we go some ways towards fixing this problem.  The results in Section \ref{sec:experiments} indicate that our posterior variance is reasonably well-calibrated.

\section{Gaussian Processes}
Gaussian processes (\gp s) offer a powerful method to perform Bayesian inference about functions \citep{GPsBook}. A \gpb is defined as a distribution over the functions $f: \Phi \rightarrow \mathbb{R}$ such that the distribution over the possible function values on any finite subset of $\Phi$ is multivariate Gaussian.  For a function $f(\lfv)$, the prior distribution over its values $\vf$ on a subset $\vlfv \subset \Phi$ is
\begin{align*}%\label{eq:\gpbDefn}
\textstyle
 &\po{\vf}\deq \N{\vf}{\vmu_{f}}{K_{f}}\\
 &\deq\frac{1}{\sqrt{\det{2\pi K_{f}}}}\,\exp \big(-\frac{1}{2}\,(\vf-\vmu_{f})\tra\,K_{f}\inv\,(\vf-\vmu_{f})\big),
\end{align*}
This distribution is implicitly conditioned on mean and covariance functions, which generate mean vector $\vmu_{f}$ and covariance matrix $K_{f}$ respectively. 
In this paper, we'll use Gaussian covariance functions,
\begin{align} \label{eq:Gaussian_cov_fn}
% K(\vlfv,\vlfv') & \deq \prod_{e=1}^{E} K_e\big(\phi\pha_e,\phi_e'\big)\\
\textstyle
K_{f}(\lfv_1,\lfv_2)& \deq h_f^2\,\N{\lfv_1}{\lfv_2}{w_{f}}.
\end{align} 
Here $h_f$ specifies the output scale (`height') over $f$, while $w_f$ defines a (squared) input scale (`width') over $\lfv$. Note that if $\lfv$ a function over multi-dimensional inputs, $w_f$ is a covariance matrix. For the remainder of the paper, we'll take $w_f$ as diagonal. 


Let us assume we have observations $(\vlfv_s,\vf_s)$ and are interested in making predictions about the function valus $f_\star$ at input $\lfv_\star$. We will assume that function inputs such as $\vlfv_s$ and $\lfv_\star$ are always known; they will not be explicitly represented. With this information, we have the predictive equations
$$
\pskinny{f\st}{\vf_s} = 
\bN{f\st}
{\meancond{f}{\vlfv_\star}{s}}
{\covcond{f}{\vlfv_\star}{s}}\,,
$$
where the mean $m$, covariance $C$, and variance $V$ are
\begin{align} 
\textstyle
&\meancond{f}{\lfv_\star}{s}
\deq \mean{f_\star}{\vf_s}
\nonumber\\
&= \mu_{f}(\lfv_\star)\!+\!
K_{f}(\lfv_\star,\vlfv_s)
K_{f}(\vlfv_s,\vlfv_s)\inv\!
\bigl(\vf_s\!-\!{\mu}_{f}(\vlfv_s)\bigr)\label{eq:GPMean}
\\[0.2cm]
&C_{f|s}(\lfv_\star, \lfv'_\star)
\deq C(f_\star,f'_\star|\vf_s) 
\nonumber\\
&= K_{f}(\lfv_\star,\lfv'_\star) - 
K_{f}(\lfv_\star,\vlfv_s)
K_{f}(\vlfv_s,\vlfv_s)\inv
K_{f}(\vlfv_s,\lfv'_\star)\,, \nonumber%\label{eq:\gpbCov}
\\[0.2cm]
&\covcond{f}{\lfv_\star}{s}
\deq {\cov{f_\star}{\vf_s}} 
\deq C_{f|s}(\lfv_\star, \lfv_\star).\nonumber
\end{align} 

\section{Bayesian Quadrature} \label{sec:bq}

% Note that maximum likelihood is also subject to issues. $\p{D}{\lfv,I}$, how come (known) $I$ is on the right and (known) $D$ is on the left? 

\emph{Bayesian quadrature} \citep{BZHermiteQuadrature,BZMonteCarlo} is a means of performing Bayesian inference about the value of a potentially nonanalytic integral 
\begin{equation} \label{eq:intyf}
 \inty{f} \deq \int f(\lfv)\,\po{\lfv}\,\ud\lfv\,.
\end{equation}
%Note that we use a condensed notation; this and all integrals to follow are definite integrals over the entire domain of interest.
We'll assume we are integrating with respect to a Gaussian prior
\begin{align}\label{eq:phiprior}
\textstyle
 \po{\lfv} & \deq \N{\lfv}{\nu_{\lfv}}{\lambda_{\lfv}} \,,
\end{align}
although other convenient forms, or, if necessary, the use of an importance re-weighting trick, allow any other integral to be approximated. If $\lfv$ is a vector, $\nu_{\lfv}$ is a  vector of identical size, and $\lambda_{\lfv}$ an appropriate covariance matrix.

%\textcolor{red}{[I'm hoping to re-write the next two paragraphs; it's way too indirect, and I think the philosophical foundations of \acro{mcmc} are a poor avenue of attack]}
Quadrature involves evaluating $f(\lfv)$ at a vector of sample points $\vlfv_s$, giving $\vf\pha_s\deq f(\vlfv_s)$. Often this evaluation is computationally expensive; the consequent sparsity of our samples introduces uncertainty about the function $f$ between them, and hence uncertainty about the integral $\inty{f}$.

%As ever in the face of uncertainty, we address the estimation of the value of our integral as a problem of Bayesian inference \citep{BZNumericalAnalysis}. 

%In considering any problem of inference, we need to be clear about both what information we have and which uncertain variables we are interested in. In our case, both the values $f(\vlfv_s)$ and their locations $\vlfv_s$ represent valuable pieces of knowledge. As discussed by \citet{MCUnsound}, traditional Monte Carlo, which approximates as
%\begin{equation} \label{eq:MC_integral_estimate}
%\inty{f} \simeq \frac{1}{\card{s}} \sum_{i=1}^{\card{s}} f(\lfv_i)\,,
%\end{equation}
%effectively ignores the information content of $\vlfv_s$, leading to unsatisfactory behaviour.
%\footnote{  For example, imagine that we had $\card{s}=3$, and $\lfv_1 = \lfv_2$. In this case, the identical value $q(\lfv_1)= q(\lfv_2)$ will receive $\nicefrac{2}{3}$ of the weight, whereas the equally useful $q(\lfv_3)$ will receive only $\nicefrac{1}{3}$. \textcolor{red}{I'm not crazy about this argument, since the weightings are also incorporating information about the prior...} }

We choose for $f$ a \gpb prior with mean $\mu_f$ and the Gaussian covariance \eqref{eq:Gaussian_cov_fn}. Here the scales $h_f$ and $w_f$ are \emph{quadrature hyperparameters}, hyperparameters that specify the  \gpb used for Bayesian quadrature. These scales are typically fitted using maximum likelihood; we will later introduce an approximate means of marginalising them.

% Many more of these will be implicitly introduced in the coming sections; we'll take this as given and incorporate them into the (hidden) context $I$. 
% Note that it will later become apparent that our inference for $\inty{f}$ is independent of the $h_f$ quadrature hyperparameter.

Note that variables over which we have a multivariate Gaussian distribution are jointly Gaussian distributed with any affine transformations of those variables. Because integration is affine, we can hence use our computed samples $\vf_s$ to perform analytic Gaussian process inference about the value of integrals over $f(\lfv)$, such as $\inty{f}$. Our mean estimate for $\inty{f}$ given $\vf_s$ is
%
\begin{align} \label{eq:mean_inty_f}
\mean{\inty{f}}{\vf_s}
& 
=\iint \inty{f}\,\p{\inty{f}}{f}\p{f}{\vf_s} \ud \inty{f} \,\ud f                                                                                                                                                               \nonumber\\
&
 =\iint \inty{f}\,\dd{\inty{f}}{\int f(\lfv)\,\po{\lfv}\,\ud\lfv}
\nonumber\\
&\hspace{2.5cm}
\N{f}{\meancondfn{f}{s}}{C_{f|s}} \ud \inty{f} \,\ud f \nonumber\\
&
 = \int \meancondfn{f}{s}(\lfv)\,\po{\lfv}\,\ud\lfv\nonumber\\
&
 = 
%\N{\inty{f}}
\mu_f + \ntT{s}{f}\, \dtt{s}{f}
%{\varpi_{f}-\ntT{s}{f} K_{f}(\vlfv_s,\vlfv_s)\inv \nt{s}{f}}
\,,
\end{align}
where for $\lfv_i \in \vlfv_s$,
\begin{align*}
\fnt{\lfv_i}{f} & \deq \!\int K_f(\lfv_i,\lfv)\po{\lfv}\ud\lfv
 = h_f^2\,\N{\lfv_i}{\nu_{\lfv}}{\lambda_{\lfv}+w_{f}}\\
%\intertext{and}
\dtt{s}{f} & \deq K_{f}\bigl(\vlfv_s,\vlfv_s\bigr)\inv (\vf_s-{\mu}_{f})\,.
\end{align*}
%
%Note that the form of our `best estimate' for $\inty{f}$, \eqref{eq:mean_inty_f}, is an affine combination of the samples $\vf_s$, just as for traditional quadrature or Monte Carlo techniques. 
% Indeed, if $\mu_f$ is taken as the mean of $\vf_s$ (as is usual for \gpb inference), the second term in \eqref{eq:mean_inty_f} can be viewed as a correction factor to the Monte Carlo estimate \eqref{eq:MC_integral_estimate}. \textcolor{red}{[Since we're using a different sampling strategy than \acro{mcmc} in this paper, I think the preceeding statement is kind of misleading / confusing...]}
% \sout{Note also that $h_f$ represents a simple multiplicative factor to both $\ntT{s}{f}$ and $K_{f}\bigl(\vlfv_s,\vlfv_s\bigr)$, and as such cancels out of \eqref{eq:mean_inty_f}.} 
%
Similarly, we can compute the means for integrals over multiple terms. In the following three sections, we will expand upon the improvements this paper introduces in the use of Bayesian Quadrature for computing model evidences.

\section{Modeling Likelihood Functions}\label{sec:model_lik}

We wish to evaluate the evidence $Z = \int \lfn(\lfv) p(\lfv) \ud\lfv$, an integral over non-negative likelihoods, $\lfn(\lfv)$, whose arguments are the parameters of the relevant model.


 \begin{figure}
 \centering
 \psfragfig[width=\columnwidth]{figures/log_transform}
 \caption{The lengthscale of a \gpb fitted to the log-likelihood function will typically be much longer than that of a \gpb fit to the likelihood function.  A \gpb with a long lengthscale will generalize better to distant parts of the function, and will have a posterior more concentrated around the true evidence. }
 \label{fig:log_is_better}
 \end{figure}

Assigning a standard \gpb prior to $\lfn(\lfv)$ would ignore our prior information about the range and non-negativity of $\lfn(\lfv)$, leading to pathologies such as potentially negative evidences (as observed in \citet{BZMonteCarlo}).  A much better prior would be a \gpb prior on $\log\bigl(\lfn(x)\bigr)$.  However, the integral under the exp of a \gpb is intractable.  In this section, we apply the approximate inference method of \citep{BQR} to tractably integrate under a log-\gpb prior,\footnote{In practice, we use the transform 
$\log\left(\nicefrac{\lfn(\lfv)}{\gamma} + 1\right)$
where $\gamma = 1$.  This give better resolution of some numerical issues, and allows us to assume the transformed quantity has zero mean. For the sake of simplicity, we omit this detail in the following derivations.}

\begin{figure}
\centering
\psfragfig{figures/delta}
\caption{An illustration of our method of finding the integral under the exp of a Gaussian process.}
\label{fig:integrate_hypers}
\end{figure}

%Firstly, we define the transformation
%\begin{align*}

% \intertext{and its inverse}
% t\inv\bigl(\tr(\lfv)\bigr) & \deq \gamma \bigl(\exp \tr(\lfv)-1\bigr)
%\end{align*}
% and their inverses
% $$
% t_q\inv\bigl(\tq(\lfv)\bigr) \deq \gamma\bigl(\eta+\exp \tq(\lfv)\bigr)
% \qquad \text{and} \qquad
% t_q\inv\bigl(\tq(\lfv)\bigr) \deq \gamma\bigl(\eta+\exp \tq(\lfv)\bigr)\,,
% $$
%(which has well-defined inverse $t\inv$) where 
%$
% \gamma \deq 100\, \max(\vr_s)
%$
%is a constant introduced for superior numerical accuracy.
%
%We now assign \gpb priors to the functions
%\begin{align*}
% \tr & \deq  t(r)
%\end{align*}
% We assume that far away from our existing observations,
% the means for $q$ and $r$ are equal to a value suitably close to
% zero, and hence take a corresponding zero prior mean for the \gp s over $\tq$ and $\tr$. 
%with Gaussian
%covariances of the form \eqref{eq:Gaussian_cov_fn}.
%the minimum of the existing observations.
%These choices of prior distribution are motivated by the fact that
%$l$ is strictly positive and possesses a large dynamic
%range. 
%
%
\begin{multline}\label{eq:minty_l}
\mean{\inty{\lfn}}{\vr_s}
% & 
% =\iint \inty{l}\,\p{\inty{l}}{l}\p{l}{\vl_s} \ud \inty{l} \,\ud l                                                                                                                                                               \nonumber\\
% &
 =\int \Bigl( \int \exp\bigl(\tr(\lfv)\bigr)\lfn(\lfv)\po{\lfv}\,\ud\lfv\Bigr)\\
\N{\tr}{\meancondfn{\tr}{s}}{C_{\tr|s}} \ud \tr\,.
\end{multline}

Here the exponential ruins the linearity that yielded the analyticity in Section \ref{sec:bq}. To combat this, we perform inference for $Z$ directly as a functional of $\tr$;
%
\begin{align*}
 Z[\tr] \deq \inty{\lfn} & = \int \exp \bigl( \tr(\lfv)\bigr) p(\lfv) \ud \lfv\\
  						    %& = \int \lfn(\lfv) p(\lfv) \ud \lfv\\
\pderiv{}{\tr(\lfv)}Z[\tr] & = \exp \bigl( \tr(\lfv)\bigr) p(\lfv) % = \lfn(\lfv) p(\lfv)
\,.
\end{align*}

We now make a \emph{linearisation} approximation\footnote{Note that this linearisation is equivalent to taking another \gpb for $Z$. with the affine covariance
\begin{equation*}
 K_Z(\tr,\tr')
% \deq 
%  K_Z\bigl((\tvq\pha_c,\tvr\pha_c),(\tvq'_c,\tvr'_c)\bigr)
\deq
\int\tr(\lfv) \tr'(\lfv) \ud \lfv
+ \omega^2\,.
\end{equation*}
} 
for $Z$, forcing $Z$ to be, as desired, affine in $\tr$. 
Note that \eqref{eq:minty_l} consists of the product of $Z[\tr]$ and a \gpb for $\tr$; the latter, due to the light tails of the Gaussian, effectively permits only a small range of $\tr$ functions. Over this narrow region, it is reasonable to assume that $Z[\tr]$ does not vary too dramatically, and can be approximated as linear. 

Before proceeding, we introduce a separate \gpb model over $\lfn$, the non-log space.  Then $m_{\lfn|s}$ is the \gpb conditional mean (as per \eqref{eq:GPMean}) for $\lfn$ given observations $\lfn(\vlfv_s)$. For this \gpb (over the non-log space), we take zero prior means and Gaussian
covariances of the form \eqref{eq:Gaussian_cov_fn}. 

We perform the linearisation of $Z[\tr]$ around the point defined by $\tr_0 \deq \log (m_{\lfn|s})$. We make the definitions 
$Z_0 \deq Z[\tr_0]$, $\pderiv{Z_0}{\tr(\lfv)} \deq \pderiv{Z}{\tr(\lfv)}[\tr_0]$, giving us the convenient mean
\begin{multline}\label{eq:linearisation}
\novmean{Z[\tr]}{Z_0,\pderiv{Z_0}{\tq(\lfv)}} 
= Z_0+\\
\int \pderiv{Z_0}{\tr(\lfv)}\bigl(\tr(\lfv)-\tr_0(\lfv)\bigr)\ud\lfv\,.
\end{multline}

% Our linearisation corresponds to giving our \gpb over $Z$ observations
% at $\tr_0= \log (m_{r|s})$ of both the functional itself,
That is, we can now write
\begin{align*}
Z_0 & = Z[\tr_0]
= 
{\mean{\inty{\lfn}}{\vr_s}}
\intertext{along with the functional derivative}
\pderiv{Z_0}{\tr(\lfv)} & = \pderiv{Z}{\tr(\lfv)}[\tr_0]
 = \mean{\lfn(\lfv)}{\vr_s}\,\po{\lfv}
\nonumber
\end{align*}
Note that $Z_0$ and its functional derivatives are analytic; our choice of $\tr_0$ permitted us to resolve the necessary integrals. We now define
\begin{align*}
\Delta & \deq m_{\tr |s} - \log(m_{l|s}) = m_{\tr |s}  - \tr_0 \,,
\end{align*}
%\textcolor{red}{[You know, until I did the notation change, I never understood that $\Delta$ was defined on the logs, not on the untransformed quantities.]} 
the difference between the \gpb mean over our log-transformed likelihood and the transformed \gpb mean over the original likelihood. 
%\textcolor{red}{[I'd like to insert here my figure 2]}. 
%we have
%\begin{align}\label{eq:mt_sim_tm}
%\Delta \simeq 0\,,
%\end{align}
%\textcolor{red}{[I found this statement confusing, it looks like we're planning to ignore $\Delta$ instead of giving it a full treatment.]}
We expect $\Delta(\lfv)$ to be everywhere small relative to the magnitude of $\tr(\phi)$ (see Figure \ref{fig:integrate_hypers}). This implies that
 $\tr_0$ is close to the peaks of our Gaussian over $\tr$, rendering our linearisation appropriate. 

For brevity, we assume that if we condition on $Z_0$, we will also implicitly condition on its functional derivatives. This allows us to write our mean for  $\inty{\lfn}$
%
\begin{align}
& \mean{\inty{\lfn}}{Z_0,\tvr_s} \nonumber\\
& \deq \int \mean{Z[\tr]}{Z_0,\tr}
\p{\tr}{\tvr_s}\, \ud \tr 
\nonumber\\
& = \mean{Z[m_{\tr|s}]}{Z_0,m_{\tr|s}} \nonumber\\
& = \mean{\inty{\lfn}}{\vr_s} + \iint \mean{\lfn(\lfv)}{\vr_s}\,
\Delta(\lfv)\,\po{\lfv}\ud\lfv
\label{eq:mean_ev1}
\end{align}

As with any linearisation approximation, \eqref{eq:mean_ev1} gives the value at the selected point $\tr_0$, plus a correction factor modeling the influence of the first derivatives. This correction factor, 
$\int \mean{\lfn(\lfv)}{\vr_s}\,
\Delta(\lfv)\,\po{\lfv}\ud\lfv$,
contains a further non-analytic integral, due to the $\tr_0$ term within $\Delta$. As such, we perform another stage of Bayesian quadrature by treating $\Delta$ as an unknown function of $\lfv$.


For this function we take Gaussian process priors with zero prior mean and Gaussian covariance \eqref{eq:Gaussian_cov_fn}. We must now choose sample points $\vlfv_c$ at which to evaluate our $\Delta$ function. 
Note that we do not need to evaluate $\lfn(\lfv_c)$ in order to compute $\Delta(\lfv_c)$.
% Unfortunately, it is impractical to resolve this decision problem in the same manner as the selection of $\vlfvS$, our ultimate goal (a topic that receives our full attention in Section \ref{sec:S\acro{bq}})).
$\vlfv_c$ should firstly include $\vlfv_s$, where we know that $\delta$ is equal to zero. Following \citet{BQR}, we select the remainder of $\vlfv_c$  as the  vertices of a Voronoi diagram \citep{okabe1997locational}, where we expect $\delta$ to be extremised. 

Given these samples, we can now marginalise \eqref{eq:mean_ev1} over $\Delta$ to give
\begin{align}
 \mean{\inty{\lfn}}{Z_0,\tvr_s,\Delta_c} =
\mean{\inty{\lfn}}{\vr_s} + \corrn \,,
\label{eq:mean_ev2}
\end{align}
where the correction factor is
\begin{align*}
 \corrn \deq \mean{\inty{\lfn \Delta}}{\vr_s, \Delta_c}\,.
\end{align*}
As $\Delta$ is small, we expect $C$ to also be small. The variance in the evidence is
\begin{align*}
& \cov{\inty{\lfn}}{Z_0,\tvr_s,\Delta_c}\\ 
& = \secm{\inty{\lfn}}{Z_0,\tvr_s} - \mean{\inty{\lfn}}{Z_0,\tvr_s,\Delta_c}^2\,,
\end{align*}
where the second moment is 
\begin{align*}
& \secm{\inty{\lfn}}{Z_0,\tvr_s}  \\
& \deq \int Z^2 
\delta\bigl(Z - \mean{Z[\tr]}{Z_0}\bigr)
\p{\tr}{\tvr_s}
\ud Z \ud\tr\\
& = \int \mean{Z[\tr]}{Z_0}^2
\N{\tr}{\meancondfn{\tr}{s}}{C_{\tr|s}}
 \ud\tr\\
& = \mean{\inty{\lfn\,C_{\tr|s}\,\lfn}}{\tvr_s}+
\mean{\inty{\lfn}}{Z_0,\tvr_s,\Delta_c}^2,
\end{align*}
and hence 
\begin{align*}
  &\cov{\inty{\lfn}}{Z_0,\tvr_s,\Delta_c} = \mean{\inty{\lfn\,C_{\tr|s}\,\lfn}}{\tvr_s} \\ 
& \deq
 \iint m_{\lfn|s}(\lfv) m_{\lfn|s}(\lfv') C_{\tr|s}(\lfv,\lfv')  p(x) p(x') \ud \lfv \ud\lfv',
\end{align*}
which is expressible in closed form, although space precludes us from doing so.




\section{Marginalising quadrature hyperparameters}
\label{sec:marginalization}
Hitherto, we have taken the maximum likelihood values $m_\theta$ for the quadrature hyperparameters $\theta$ of the \gpb used to model the integrand, $\tr$. Once we have very many samples, this approximation seems reasonable: with lots of data, we can be almost certain about the values of the quadrature hyperparameters. However, we want to consider actively selecting samples, guided by our beliefs about the integrand: we require this process to be robust even when we have only few samples. 


\begin{figure}
\centering
\psfragfig[width=0.5\textwidth]{figures/int_hypers}
\caption{A demo of the effect of integrating hyperparameters on the marginal posterior variance.}
\label{fig:integrate_hypers}
\end{figure}

The hyperparameters of most interest are the input scales $w_\tr$; these hyperparameters can have a very dramatic influence on our fit to a function. Unfortunately, the dependence of our predictions upon these input scales is complex, ruling out analytic results. However, any approximation we make is likely to improve upon our existing assumption that the posterior for all hyperparameters is a delta function! We make the following assumptions.

 \paragraph*{Flat prior:} We assume that the prior for $\theta$ is very broad, so that our posterior is a simple scaling of the likelihood. 
\paragraph*{Laplace approximation:} The likelihood  is taken as Gaussian with mean equal to the maximum likelihood value $m_\theta$, and with diagonal covariance matrix $C_\theta$. For quadrature hyperparameters other than the input scales, we take the appropriate diagonal elements as infinitesimally small, such that our posterior for these parameters is a delta function. The diagonal elements corresponding to $w_\tr$ are fitted using the Hessian of the likelihood.
%the squared input scales $w_s$ of another \gp, fitted to the likelihood $s(w_r)$ of the input scales $w_r$. \textcolor{red}{[Neat idea, but I can think of simple examples where the lengthscale is short but the mass is spread out; it seems to me that it'd be better to first fit a \gpb, then fit a Gaussian to the \gpb posterior.]}
\paragraph*{Affine GP mean:} Our predictions for $\tr$ are assumed to have a \gpb mean which is affine in $w_\tr$ around the maximum likelihood values, and a constant variance.  

 
The implication of these assumptions is that the posterior mean over $\tr$ remains the same. However, the posterior variance is inflated by an additive factor
$
(\npderiv{m_{\tr|s}}{\theta})\,C_\theta\,(\npderiv{m\tra_{\tr|s}}{\theta})
$. 
 % David sez: I moved this to the appendix.
%With these assumptions, we have
%\begin{align*}
%& \mean{\inty{\lfn}}{Z_0,\tvr_s} \\
%& \deq \iint \mean{Z[\tr]}{Z_0,\tr}
%\p{\tr}{\tvr_s, \theta}\, \p{\theta}{\tvr_s}\ud \tr\,\ud \theta\\
%& = \iint \mean{Z[\tr]}{Z_0,\tr} \N{\tr}{m_{\tr|s,\theta}}{C_{\tr|s,\theta}}
%\\
%& \hspace{5cm}\N{\theta}{m_\theta}{C_\theta}\ud \tr\,\ud \theta
%\\
%& \simeq \iint \mean{Z[\tr]}{Z_0,\tr} 
%\\
%& \hspace{2cm}\N{\tr}
%{m_{\tr|s,m_\theta}+\pderiv{m_{\tr|s,\theta}}{\theta}(\theta-m_\theta)}
%{C_{\tr|s,m_\theta}}
%\\
%& \hspace{4cm}
%\N{\theta}{m_\theta}{C_\theta}\ud \tr\,\ud \theta
%\\
%& = \iint \mean{Z[\tr]}{Z_0,\tr} \\
%& \hspace{1cm}\N{\tr}
%{m_{\tr|s,m_\theta}}
%{C_{\tr|s,m_\theta}+\pderiv{m_{\tr|s,\theta}}{\theta}C_\theta\pderiv{m\tra_{\tr|s,\theta}}{\theta}}
%\ud \tr
%\\
%& = \mean{Z[\tr]}{Z_0,m_{\tr|s,m_\theta}}\,,
%\end{align*}
%which is identical to \eqref{eq:mean_ev} (note that we had previously just implicitly assumed that $\theta=m_\theta$). 
This implies that our posterior mean for $Z$ is unchanged, and the variance becomes
\begin{align*}
  &\cov{\inty{\lfn}}{Z_0,\tvr_s,\Delta_c}=
 \iint m_{\lfn|s}(\lfv)\, m_{\lfn|s}(\lfv')\\
&  
\hspace{0.5cm}\biggl(C_{\tr|s}(\lfv,\lfv') + \pderiv{m_{\tr|s}}{\theta}\,C_\theta\,\pderiv{m\tra_{\tr|s}}{\theta}\biggr)
 \ud \lfv \ud\lfv',
\end{align*}
which is again possible, although laborious, to express analytically.



\section{Doubly Bayesian Quadrature}

We aim to select samples $\lfv_s$ so as to minimise the \textit{expected} uncertainty in the evidence after seeing our next sample.\footnote{We also expect such samples to be useful not just for estimating the evidence, but also for any other related expectations, such as would be required to perform prediction using the model.}
Surprisingly, the posterior variance of a \gpb model with fixed hyperparameters does not depend on the function values at sampled locations all all; only the location of those samples matters. The evidence is a simple affine transformation of the likelihood, which traditional Bayesian quadrature models with a \gp; hence its estimate for the variance in the evidence is likewise independent of likelihood values. As such, active learning is pointless, and the optimal sampling design can be found in advance \cite{minka2000dqr}.

In Section \ref{sec:model_lik}, we took the integral as the affine transform of the log-likelihood, which we model with a \gpb. As the affine transformation itself depends on the function values (see \eqref{eq:linearisation}), active learning is now desirable. Our uncertainty over the hyperparameters of the \gpb further motivates active learning: without assuming \textit{a priori} knowledge of the hyperparameters, we can't evaluate the \gpb to compute a sampling schedule beforehand. Our approximate marginalisation of hyperparameters permits us an approach to active sampling that acknowledges the influence new samples may have on our beliefs about the hyperparameters. We'd expect this would further promote exploration, particularly early on, so as to firm up our beliefs about the hyperparameters. 

 \begin{figure}
 \centering
\psfragfig[width=0.5\textwidth]{figures/eue_progression}
 \caption{An example showing the expected uncertainty in the evidence after observing the likelihood function at that location.}
 \label{fig:eue}
 \end{figure}
 
%\begin{figure}
%\centering
%\includegraphics[width=0.48\textwidth]{figures/active_learning.eps}
%\caption{An example of the posterior over likelihood functions converging as new samples are selected.}
%\label{fig:active_learning}
%\end{figure}

\begin{figure}
\centering
\psfragfig{figures/plots/sampleplot_two_spikes_1d}
\caption{The location of samples chosen by different methods.}
\label{fig:sample_paths}
\end{figure}

In selecting a new sample $\lfv_a$, we wish to minimise the variance in our evidence after adding it to our model. As such, we choose that variance as our loss function, and choose the $\lfv_a$ that minimises the expected loss
\begin{align}
&\bigl\langle \cov{\inty{\lfn}}{Z_0,\tvr_{s,a}}\mid Z_0,\tvr_{s}\bigr\rangle 
\nonumber\\
% & = \secm{\inty{\lfn}}{Z_0,\tvr_s} 
% - \int\mean{\inty{\lfn}}{Z_0,\tvr_{a,s},\Delta_c}^2\nonumber\\
% & \hspace{3.8cm} \p{\tr_a}{\tvr_s}\ud\tr_a\,.\nonumber\\
 & = \secm{\inty{\lfn}}{Z_0,\tvr_s} 
 - \int\mean{\inty{\lfn}}{Z_0,\tvr_{a,s},\Delta_c}^2\nonumber\\
& \hspace{0.2cm}
\times\N{\tr_a}
{\tilde{m}_a }
{\tilde{C}_a +\pderiv{\tilde{m}_a}{\theta}C_\theta\pderiv{\tilde{m}\tra_a}{\theta}}
\ud\tr_a\,.\label{eq:exp_var}
\end{align}
where
\begin{align*}
\tilde{m}_a & \deq \mean{\tr_a}{\tvr_s,m_\theta}\\
\tilde{C}_a & \deq \cov{\tr_a}{\tvr_s,m_\theta}\,.
\end{align*}
The first term in \eqref{eq:exp_var}, the (expected) second moment, is independent of the selection of $\lfv_a$ and hence can be safely ignored for active sampling. This is true regardless of the probabilistic model chosen for the likelihood surface. 
Noting that $\int \exp(c\, y)\, \N{y}{m}{\sigma^2} \ud y = \exp(c\, m + \nicefrac{1}{2}\, c^2 \sigma^2)$, 
the second term, the negative expected squared mean, can be resolved analytically\footnote{We assume that $\Delta$ does not depend on $\tr_a$, only its location $x_a$: we know $\Delta(x_a) = 0$ and assume $\Delta$ elsewhere remains relatively unchanged.}
 for any trial $\lfv_a$ (we omit the laborious details of doing so). We do not have to make a linearisation approximation here, and our prior over $\tr_a$ can be fully exploited for its most important purpose: selecting new samples. 

In order to minimize the expected variance, we are hence impelled to maximize the expected squared mean. Unlike in traditional Bayesian quadrature, our exponentiation means that one means of doing so is to seek points where the log-likelihood is predicted to be large: exploitation. We are also compelled to seek out points where our current variance in the log-likelihood is significant, leading to exploration. Note that our variance for $\tr_a$ has been inflated due to our consideration of the influence of hyperparameters (See Figure \ref{fig:integrate_hypers}), which we expect to promote additional exploration, as desired.




\section{Experiments}
\label{sec:experiments}
In this section we attempt to show that our changes to the \acro{bmc} algorithm improve its convergence to the truth.  We also compare against several standard \acro{mcmc} methods.
\paragraph{Integrands}
We chose a set of problems which would demonstrate the strengths and weakness of the various methods we compared.
\input{tables/integrands.tex}
The true value of $Z$ for the non-analytic integrands was estimated by a run of \acro{smc} with $10^6$ samples.
\paragraph{Methods}
\begin{itemize}
\item Simple Monte Carlo (\acro{smc})
\item Annealed Importance Sampling (\acro{ais}).  The inverse temperature schedule was linear as in \citep{BZMonteCarlo}, and the proposal width was adjusted to attain approximately a 50\% acceptance rate.  In an attempt to be fair, the posterior variance of the \acro{ais} estimate of $Z$ was set to the empirical variance over 10 chains.
\item Bayesian Monte Carlo (\acro{bmc}) - the algorithm used in \citep{BZMonteCarlo}, in which samples were chosen from a \acro{ais} chain, and a \gpb was fit to the likelihood samples.
\item Bayesian Quadrature (\acro{bq}) - Bayesian Quadrature using a \gpb on the log-transformed lieklihood values, but not marginalizing over hyperparameters, and selecting sample locations via \acro{ais}.
\item Doubly Bayesian Quadrature (\acro{bbq}) - The algorithm outlined in this paper, in which samples are chosen actively, lengthscale hyperparameters are approximately integrated out, and a \gpb was defined over the log-transformed likelihood values.
\end{itemize}
We minimise \eqref{eq:exp_var} using a multi-start local optimizer, where the starting points are chosen as $\lfv_c$. 

One somewhat standard method that is missing from this comparison is Hamiltonian Monte Carlo, which uses information about the derivatives in order to speed mixing.  However, Bayesian Quadrature can also incorporate information about the derivatives of the likelihood function, and would presumably also perform better if given this information.  An exhaustive comparison with \acro{mcmc} methods is beyond the scope of this paper.

\paragraph{Evaluation}

 \begin{figure}
 \centering
 \begin{tabular}{ccc}
 	\psfragfig{figures/integrands/easy_1d} &
 	\psfragfig{figures/integrands/bumpy_1d} 
 	\psfragfig{figures/integrands/two_spikes_1d}
 \end{tabular}
 \caption{The one-dimensional integrands in our test suite.  Green dashed lines are priors, blue lines are likelihoods.
 % \textcolor{red}{mike says: maybe we should make the lines variously dashed to help with B\&W printing. Also I'm not sure we need the posterior, which could be removed to de-clutter.}
 }
 \label{fig:1d_problems}
 \end{figure}
% 
% %\begin{figure}
% %\centering
% %\includegraphics[width=0.45\textwidth]{figures/integrands/funnel.eps}
% %\caption{Radford Neal's funnel problem in 2 dimensions.}
% %\label{fig:funnel}
% %\end{figure}
% 
% %\begin{minipage}[t][0.45\paperheight][t]{0.45\paperwidth}
% %    \input{tables/times_taken.tex}
% %\end{minipage}
% 
% 
%\begin{figure}
%	\centering
%\end{figure}

\begin{figure}
	\centering
	\psfragfig[width=6cm,height=4cm]{figures/plots/log_of_truth_plot_simple} 	
	\psfragfig[width=3cm]{figures/plots/legend}
	\caption{The negative log density of the true normalization constant, versus different methods.}
\end{figure}

\begin{figure}
	\centering
	\psfragfig[width=8cm,height=6cm]{figures/plots/varplot_easy_4d}
	\caption{The posterior mean and variance over Z for different methods, compared to the true Z (in black)}
\end{figure}




\section{Discussion}

Adjusting the number of candidate points allows one to trade off between computational speed and number of samples required.

While we used an active learning scheme in this paper, Bayesian Quadrature can be used on samples generated by any method.

\paragraph{Future Work}There are many ways to improve the \gpb model used to model likelihood functions.  First and foremost, the Gaussian kernel used in this paper has very weak generalization abilities.  Allowing a richer class of kernels will allow the \gpb model to shrink its posterior much more quickly.  For example, log-likelihood functions typically take the form of a sum of many terms, each depending only on a small number of variables.  This prior information could easily be incorporated into the \gp, presumably allowing the \acro{bbq} algorithm to form concentrated posterior distributions over high-dimensional likelihoods.

The methods developed here can be applied without modification to discrete domains.

\section{Conclusions}

 In this paper, we have made several advances to the \acro{bmc} method.  We have also demonstrated the many advantages of model-based integration over standard \acro{mcmc}: a natural stopping criterion, an estimate of uncertainty in our integral, and the ability to use active learning to select our samples rather than Monte Carlo methods.

\acro{mcmc} is an extremely widely used method, and advances in \acro{mcmc} allow modelers [such as...] to use richer model classes and to be more confident in their model evaluations.  While we do not expect model-based integration approaches to be a better choice in every instance, we expect that this family of approaches will one day be seen as a standard alternative to \acro{mcmc}.

\section*{Acknowledgements}

\bibliography{bub}
\bibliographystyle{icml2012}
\pagebreak
\input{tables/truth_prob.tex}
\input{tables/se.tex}

\end{document} 


